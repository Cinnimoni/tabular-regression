{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:29:31.974114Z",
     "iopub.status.busy": "2023-03-07T18:29:31.973632Z",
     "iopub.status.idle": "2023-03-07T18:29:31.987130Z",
     "shell.execute_reply": "2023-03-07T18:29:31.985427Z",
     "shell.execute_reply.started": "2023-03-07T18:29:31.974074Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from sklearn import svm\n",
    "import lightgbm as ltb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:14:37.821954Z",
     "iopub.status.busy": "2023-03-07T18:14:37.820392Z",
     "iopub.status.idle": "2023-03-07T18:14:37.875903Z",
     "shell.execute_reply": "2023-03-07T18:14:37.874933Z",
     "shell.execute_reply.started": "2023-03-07T18:14:37.821888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "      <td>5407.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>299.168189</td>\n",
       "      <td>58.610579</td>\n",
       "      <td>31.872795</td>\n",
       "      <td>185.076235</td>\n",
       "      <td>4.108441</td>\n",
       "      <td>992.000718</td>\n",
       "      <td>771.219974</td>\n",
       "      <td>51.751618</td>\n",
       "      <td>35.452071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>105.537682</td>\n",
       "      <td>83.417801</td>\n",
       "      <td>54.605003</td>\n",
       "      <td>18.517583</td>\n",
       "      <td>5.692296</td>\n",
       "      <td>77.148010</td>\n",
       "      <td>78.725253</td>\n",
       "      <td>70.006975</td>\n",
       "      <td>16.401896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>213.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>175.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>938.200000</td>\n",
       "      <td>734.300000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>297.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>187.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>978.000000</td>\n",
       "      <td>781.200000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.000000</td>\n",
       "      <td>122.600000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>1047.000000</td>\n",
       "      <td>821.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>45.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "count      5407.000000       5407.000000      5407.000000     5407.000000   \n",
       "mean        299.168189         58.610579        31.872795      185.076235   \n",
       "std         105.537682         83.417801        54.605003       18.517583   \n",
       "min         102.000000          0.000000         0.000000      121.800000   \n",
       "25%         213.700000          0.000000         0.000000      175.100000   \n",
       "50%         297.200000          0.000000         0.000000      187.400000   \n",
       "75%         375.000000        122.600000        79.000000      192.000000   \n",
       "max         540.000000        359.400000       200.100000      247.000000   \n",
       "\n",
       "       SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "count                5407.000000               5407.000000   \n",
       "mean                    4.108441                992.000718   \n",
       "std                     5.692296                 77.148010   \n",
       "min                     0.000000                801.000000   \n",
       "25%                     0.000000                938.200000   \n",
       "50%                     0.000000                978.000000   \n",
       "75%                     8.050000               1047.000000   \n",
       "max                    32.200000               1145.000000   \n",
       "\n",
       "       FineAggregateComponent    AgeInDays     Strength  \n",
       "count             5407.000000  5407.000000  5407.000000  \n",
       "mean               771.219974    51.751618    35.452071  \n",
       "std                 78.725253    70.006975    16.401896  \n",
       "min                594.000000     1.000000     2.330000  \n",
       "25%                734.300000     7.000000    23.640000  \n",
       "50%                781.200000    28.000000    33.950000  \n",
       "75%                821.000000    56.000000    45.850000  \n",
       "max                992.600000   365.000000    82.600000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv').set_index('id')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(abs(df.corr()), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:10:10.655658Z",
     "iopub.status.busy": "2023-03-07T18:10:10.654419Z",
     "iopub.status.idle": "2023-03-07T18:10:10.711694Z",
     "shell.execute_reply": "2023-03-07T18:10:10.710250Z",
     "shell.execute_reply.started": "2023-03-07T18:10:10.655608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "      <td>3605.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>299.943523</td>\n",
       "      <td>55.803273</td>\n",
       "      <td>32.829154</td>\n",
       "      <td>184.345825</td>\n",
       "      <td>4.194230</td>\n",
       "      <td>990.222330</td>\n",
       "      <td>774.736311</td>\n",
       "      <td>49.153398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.809283</td>\n",
       "      <td>80.938863</td>\n",
       "      <td>55.490684</td>\n",
       "      <td>17.964842</td>\n",
       "      <td>5.807567</td>\n",
       "      <td>76.812197</td>\n",
       "      <td>77.412505</td>\n",
       "      <td>63.564334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>213.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>174.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>938.200000</td>\n",
       "      <td>739.300000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>977.600000</td>\n",
       "      <td>781.500000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>374.000000</td>\n",
       "      <td>117.200000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>1047.000000</td>\n",
       "      <td>821.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "count      3605.000000       3605.000000      3605.000000     3605.000000   \n",
       "mean        299.943523         55.803273        32.829154      184.345825   \n",
       "std         104.809283         80.938863        55.490684       17.964842   \n",
       "min         102.000000          0.000000         0.000000      121.800000   \n",
       "25%         213.800000          0.000000         0.000000      174.900000   \n",
       "50%         300.000000          0.000000         0.000000      188.500000   \n",
       "75%         374.000000        117.200000        90.000000      192.000000   \n",
       "max         540.000000        359.400000       200.100000      247.000000   \n",
       "\n",
       "       SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "count                3605.000000               3605.000000   \n",
       "mean                    4.194230                990.222330   \n",
       "std                     5.807567                 76.812197   \n",
       "min                     0.000000                801.000000   \n",
       "25%                     0.000000                938.200000   \n",
       "50%                     0.000000                977.600000   \n",
       "75%                     8.100000               1047.000000   \n",
       "max                    32.200000               1145.000000   \n",
       "\n",
       "       FineAggregateComponent    AgeInDays  \n",
       "count             3605.000000  3605.000000  \n",
       "mean               774.736311    49.153398  \n",
       "std                 77.412505    63.564334  \n",
       "min                594.000000     1.000000  \n",
       "25%                739.300000     7.000000  \n",
       "50%                781.500000    28.000000  \n",
       "75%                821.000000    56.000000  \n",
       "max                992.600000   365.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv').set_index('id')\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:10:10.715311Z",
     "iopub.status.busy": "2023-03-07T18:10:10.714931Z",
     "iopub.status.idle": "2023-03-07T18:10:10.722941Z",
     "shell.execute_reply": "2023-03-07T18:10:10.721701Z",
     "shell.execute_reply.started": "2023-03-07T18:10:10.715273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#checking for any nan values\n",
    "print(df.isnull().values.any())\n",
    "print(df_test.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:10:10.724900Z",
     "iopub.status.busy": "2023-03-07T18:10:10.724543Z",
     "iopub.status.idle": "2023-03-07T18:10:10.734724Z",
     "shell.execute_reply": "2023-03-07T18:10:10.733570Z",
     "shell.execute_reply.started": "2023-03-07T18:10:10.724865Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('Strength',axis=1)\n",
    "y = df['Strength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:10:10.754857Z",
     "iopub.status.busy": "2023-03-07T18:10:10.754141Z",
     "iopub.status.idle": "2023-03-07T18:10:10.774793Z",
     "shell.execute_reply": "2023-03-07T18:10:10.773450Z",
     "shell.execute_reply.started": "2023-03-07T18:10:10.754807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CementComponent</th>\n",
       "      <th>BlastFurnaceSlag</th>\n",
       "      <th>FlyAshComponent</th>\n",
       "      <th>WaterComponent</th>\n",
       "      <th>SuperplasticizerComponent</th>\n",
       "      <th>CoarseAggregateComponent</th>\n",
       "      <th>FineAggregateComponent</th>\n",
       "      <th>AgeInDays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>166.1</td>\n",
       "      <td>75.4</td>\n",
       "      <td>163.8</td>\n",
       "      <td>173.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>746.6</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>304.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>801.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>251.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.3</td>\n",
       "      <td>188.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1028.4</td>\n",
       "      <td>757.7</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5411</th>\n",
       "      <td>144.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>709.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CementComponent  BlastFurnaceSlag  FlyAshComponent  WaterComponent  \\\n",
       "id                                                                         \n",
       "5407            166.1              75.4            163.8           173.8   \n",
       "5408            304.0               0.0              0.0           190.0   \n",
       "5409            225.0               0.0              0.0           185.0   \n",
       "5410            251.4               0.0            118.3           188.5   \n",
       "5411            144.0              15.0            195.0           176.0   \n",
       "\n",
       "      SuperplasticizerComponent  CoarseAggregateComponent  \\\n",
       "id                                                          \n",
       "5407                        4.6                    1007.2   \n",
       "5408                        0.0                     998.0   \n",
       "5409                        0.0                    1113.0   \n",
       "5410                        6.4                    1028.4   \n",
       "5411                        6.0                    1021.0   \n",
       "\n",
       "      FineAggregateComponent  AgeInDays  \n",
       "id                                       \n",
       "5407                   746.6         56  \n",
       "5408                   801.0          7  \n",
       "5409                   833.0         28  \n",
       "5410                   757.7        100  \n",
       "5411                   709.0         28  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = df_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:29:35.893793Z",
     "iopub.status.busy": "2023-03-07T18:29:35.893260Z",
     "iopub.status.idle": "2023-03-07T18:34:15.199191Z",
     "shell.execute_reply": "2023-03-07T18:34:15.197455Z",
     "shell.execute_reply.started": "2023-03-07T18:29:35.893742Z"
    }
   },
   "outputs": [],
   "source": [
    "# def xbg_opt(x):\n",
    "#     #est = x.suggest_int('n_estimators', 1500, 3000)\n",
    "#     learning_rate = x.suggest_float('learning_rate', 0, 0.008)\n",
    "#     subsample = x.suggest_float('subsample', 0.6, 1)\n",
    "#     depth = x.suggest_int('max_depth', 3, 10)\n",
    "#     xgb = XGBRegressor(n_estimators=3000, max_depth=depth,\n",
    "#                        eta=learning_rate, subsample = subsample)\n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "#     xgb.fit(X_train, y_train)\n",
    "#     xgb_preds = xgb.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, xgb_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(xbg_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsXGB = []\n",
    "predsXGB = []\n",
    "xgbr_params = dict(n_estimators=3000, max_depth=4,\n",
    "                   eta=0.0016871856953315112, subsample = 0.7198403750944147,\n",
    "                   tree_method = 'gpu_hist')\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    xgb = XGBRegressor(**xgbr_params)\n",
    "    \n",
    "    xgb.fit(X=X_train, y=y_train, eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds = 100, verbose = False)\n",
    "\n",
    "    xgb_preds = xgb.predict(X_test)\n",
    "    \n",
    "    modelsXGB.append(xgb)\n",
    "    predsXGB.append(xgb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cb_opt(x):\n",
    "#     # iterations = x.suggest_int('iterations', 1500, 3000)\n",
    "#     learning_rate = x.suggest_float('learning_rate', 0, 0.05)\n",
    "#     subsample = x.suggest_float('subsample', 0.5, 1)\n",
    "#     # depth =  x.suggest_int('depth', 3, 10)\n",
    "#     reg = x.suggest_float('reg', 0, 1)\n",
    "#     cb = CatBoostRegressor(iterations=2500, learning_rate=learning_rate, \n",
    "#                            depth=5, verbose=False, subsample = subsample, \n",
    "#                            l2_leaf_reg=reg)\n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "#     cb.fit(X_train, y_train)\n",
    "#     cb_preds = cb.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, cb_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(cb_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsCB = []\n",
    "predsCB = []\n",
    "cbr_params = dict(iterations=2500, max_depth=5,\n",
    "                  learning_rate = 0.0033882877566152616, subsample = 0.8953685117189872,\n",
    "                  task_type = 'GPU', l2_leaf_reg = 0.714399266280004, verbose = False, \n",
    "                  bootstrap_type='Poisson')\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    cb = CatBoostRegressor(**cbr_params)\n",
    "    \n",
    "    cb.fit(X=X_train, y=y_train, eval_set=[(X_valid, y_valid)],\n",
    "    early_stopping_rounds = 100)\n",
    "\n",
    "    cb_preds = cb.predict(X_test)\n",
    "    \n",
    "    modelsCB.append(cb)\n",
    "    predsCB.append(cb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rf_opt(x):\n",
    "#     depth = x.suggest_int('max_depth', 5, 10)\n",
    "#     max_features = x.suggest_int('max_features', 2, 8)\n",
    "#     min_samples_leaf = x.suggest_int('min_samples_leaf', 5, 50)\n",
    "#     rf = RandomForestRegressor(n_estimators = 2000, criterion = 'squared_error', max_depth = depth, \n",
    "#                                max_features = max_features, min_samples_leaf = min_samples_leaf)\n",
    "    \n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "#     rf.fit(X_train, y_train)\n",
    "#     rf_preds = rf.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, rf_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(rf_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "rfr_params = dict(n_estimators=2000, max_depth=6,\n",
    "                  max_features = 7, verbose = False, \n",
    "                  min_samples_leaf = 15)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "  \n",
    "    rf = RandomForestRegressor(**rfr_params)\n",
    "    \n",
    "    rf.fit(X=X_train, y=y_train)\n",
    "\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    \n",
    "    modelsRF.append(rf)\n",
    "    predsRF.append(rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lasso_opt(x):\n",
    "#     eps = x.suggest_float('eps', 0, 0.001)\n",
    "#     cv = x.suggest_int('cv', 2, 5)\n",
    "#     n_alphas = x.suggest_int('n_alphas', 1000, 2000)\n",
    "#     alpha = x.suggest_float('alpha', 0, 0.1)\n",
    "#     tol = x.suggest_float('tol', 0, 0.01)\n",
    "#     lasso = LassoCV(precompute = 'auto', fit_intercept = True,\n",
    "#                     normalize = False, max_iter = 10000, eps = eps, \n",
    "#                     cv = cv, n_alphas = n_alphas, n_jobs = 8,\n",
    "#                     tol = tol)\n",
    "#     lasso_par = {\n",
    "#          'alpha': alpha,\n",
    "#          'max_iter': 10000,\n",
    "#          'tol': tol    \n",
    "#      }\n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    \n",
    "#     lasso.fit(X_train, y_train)\n",
    "#     lasso_preds = lasso.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, lasso_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(lasso_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsLasso = []\n",
    "predsLasso = []\n",
    "\n",
    "lasso_params = dict(alpha = 0.011331309463411023, max_iter = 10000, tol = 0.009713287632154238)\n",
    "\n",
    "lassocv_params = dict(precompute = 'auto', fit_intercept = True,\n",
    "                    normalize = False, max_iter = 10000, eps = 0.00043732786136795276, \n",
    "                    cv = 5, n_alphas = 1742, n_jobs = 8,\n",
    "                    tol = 0.009713287632154238)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "  \n",
    "    lasso = Lasso(**lasso_params)\n",
    "    \n",
    "    lasso.fit(X=X_train, y=y_train)\n",
    "\n",
    "    lasso_preds = lasso.predict(X_test)\n",
    "    \n",
    "    modelsLasso.append(lasso)\n",
    "    predsLasso.append(lasso_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SVR_opt(x):\n",
    "#     tol = x.suggest_float('tol', 0, 0.001)\n",
    "#     C = x.suggest_float('C', 0.98, 1.05)\n",
    "#     eps = x.suggest_float('epsilon', 0.005, 0.02)\n",
    "#     SVR = svm.SVR(tol = tol, C = C, epsilon = eps, gamma = 0.001)\n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    \n",
    "#     SVR.fit(X_train, y_train)\n",
    "#     svr_preds = SVR.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, svr_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(SVR_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsSVR = []\n",
    "predsSVR = []\n",
    "\n",
    "\n",
    "svr_params = dict(tol = 0.0007618119753298281, C = 1.049996968413195, \n",
    "                  epsilon = 0.015552737698416759, gamma = 0.001)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "  \n",
    "    SVR = svm.SVR(**svr_params)\n",
    "    \n",
    "    SVR.fit(X=X_train, y=y_train)\n",
    "\n",
    "    SVR_preds = SVR.predict(X_test)\n",
    "    \n",
    "    modelsSVR.append(SVR)\n",
    "    predsSVR.append(SVR_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LGBM_opt(x):\n",
    "#     learning_rate = x.suggest_float('learning_rate', 0.001, 0.01)\n",
    "#     #reg_alpha = x.suggest_float('reg_alpha', 0.001, 0.015)\n",
    "#     #reg_lambda = x.suggest_float('reg_lambda', 0.001, 0.015)\n",
    "#     #num_leaves = x.suggest_int('num_leaves', 5, 20)\n",
    "#     subsample = x.suggest_float('subsample', 0.5, 1)\n",
    "#     lambda_l1 = x.suggest_int('lambda_l1', 1, 6)\n",
    "#     lambda_l2 = x.suggest_int('lambda_l2', 1, 6)\n",
    "#     LGBM = ltb.LGBMRegressor(n_estimators = 1000, num_leaves = 9, learning_rate = learning_rate,\n",
    "#                              lambda_l1 = lambda_l1, lambda_l2 = lambda_l2 , max_depth = 10,\n",
    "#                              subsample = subsample, feature_fraction = 0.8, bagging_fraction = 0.8,\n",
    "#                              device = 'gpu')\n",
    "    \n",
    "#     X_train, X_true, y_train, y_true = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "     \n",
    "#     LGBM.fit(X_train, y_train)\n",
    "#     lgbm_preds = LGBM.predict(X_true)\n",
    "\n",
    "#     return mean_squared_error(y_true, lgbm_preds, squared = False)\n",
    "\n",
    "# study = optuna.create_study(direction= 'minimize')\n",
    "# study.optimize(LGBM_opt, n_trials= 100)\n",
    "\n",
    "# study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.9410521882109187 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=3, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.9410521882109187 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=3, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.9410521882109187 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=3, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.9410521882109187 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=3, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l1 is set=4, reg_alpha=0.0 will be ignored. Current value: lambda_l1=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=0.9410521882109187 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=3, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=5)\n",
    "\n",
    "modelsLGBM = []\n",
    "predsLGBM = []\n",
    "\n",
    "\n",
    "lgbm_params = dict(n_estimators = 1000, num_leaves = 9, learning_rate = 0.009121343086184033,\n",
    "                             lambda_l1 = 4, lambda_l2 = 3 , max_depth = 10,\n",
    "                             subsample = 0.9410521882109187, feature_fraction = 0.8, bagging_fraction = 0.8,\n",
    "                             device = 'gpu')\n",
    "\n",
    "for train_index, test_index in k_fold.split(X,y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "  \n",
    "    LGBM = ltb.LGBMRegressor(**lgbm_params)\n",
    "    \n",
    "    LGBM.fit(X=X_train, y=y_train, eval_set=[(X_valid, y_valid)],\n",
    "          early_stopping_rounds = 100, verbose = False)\n",
    "\n",
    "    LGBM_preds = LGBM.predict(X_test)\n",
    "    \n",
    "    modelsLGBM.append(LGBM)\n",
    "    predsLGBM.append(LGBM_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1152      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166,017\n",
      "Trainable params: 166,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 10, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 12.5610 - mean_absolute_error: 12.5610\n",
      "Epoch 1: val_loss improved from inf to 12.26200, saving model to Weights-1--12.26200.hdf5\n",
      "136/136 [==============================] - 1s 3ms/step - loss: 12.3642 - mean_absolute_error: 12.3642 - val_loss: 12.2620 - val_mean_absolute_error: 12.2620\n",
      "Epoch 2/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 10.9337 - mean_absolute_error: 10.9337\n",
      "Epoch 2: val_loss improved from 12.26200 to 10.95823, saving model to Weights-2--10.95823.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 10.9184 - mean_absolute_error: 10.9184 - val_loss: 10.9582 - val_mean_absolute_error: 10.9582\n",
      "Epoch 3/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 10.3720 - mean_absolute_error: 10.3720\n",
      "Epoch 3: val_loss improved from 10.95823 to 10.54099, saving model to Weights-3--10.54099.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 10.2972 - mean_absolute_error: 10.2972 - val_loss: 10.5410 - val_mean_absolute_error: 10.5410\n",
      "Epoch 4/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.9978 - mean_absolute_error: 9.9978  \n",
      "Epoch 4: val_loss did not improve from 10.54099\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 10.0931 - mean_absolute_error: 10.0931 - val_loss: 10.9855 - val_mean_absolute_error: 10.9855\n",
      "Epoch 5/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 9.7489 - mean_absolute_error: 9.7489\n",
      "Epoch 5: val_loss improved from 10.54099 to 10.06694, saving model to Weights-5--10.06694.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.7684 - mean_absolute_error: 9.7684 - val_loss: 10.0669 - val_mean_absolute_error: 10.0669\n",
      "Epoch 6/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 9.4680 - mean_absolute_error: 9.4680\n",
      "Epoch 6: val_loss did not improve from 10.06694\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5105 - mean_absolute_error: 9.5105 - val_loss: 10.2028 - val_mean_absolute_error: 10.2028\n",
      "Epoch 7/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.5537 - mean_absolute_error: 9.5537\n",
      "Epoch 7: val_loss improved from 10.06694 to 10.00767, saving model to Weights-7--10.00767.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.6014 - mean_absolute_error: 9.6014 - val_loss: 10.0077 - val_mean_absolute_error: 10.0077\n",
      "Epoch 8/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 9.7423 - mean_absolute_error: 9.7423\n",
      "Epoch 8: val_loss did not improve from 10.00767\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.6888 - mean_absolute_error: 9.6888 - val_loss: 10.5641 - val_mean_absolute_error: 10.5641\n",
      "Epoch 9/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.4134 - mean_absolute_error: 9.4134\n",
      "Epoch 9: val_loss improved from 10.00767 to 9.82375, saving model to Weights-9--9.82375.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4654 - mean_absolute_error: 9.4654 - val_loss: 9.8238 - val_mean_absolute_error: 9.8238\n",
      "Epoch 10/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.4395 - mean_absolute_error: 9.4395\n",
      "Epoch 10: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4763 - mean_absolute_error: 9.4763 - val_loss: 11.0288 - val_mean_absolute_error: 11.0288\n",
      "Epoch 11/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.4807 - mean_absolute_error: 9.4807\n",
      "Epoch 11: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4986 - mean_absolute_error: 9.4986 - val_loss: 11.0959 - val_mean_absolute_error: 11.0959\n",
      "Epoch 12/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.5065 - mean_absolute_error: 9.5065\n",
      "Epoch 12: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5404 - mean_absolute_error: 9.5404 - val_loss: 10.1614 - val_mean_absolute_error: 10.1614\n",
      "Epoch 13/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 9.5147 - mean_absolute_error: 9.5147\n",
      "Epoch 13: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5630 - mean_absolute_error: 9.5630 - val_loss: 12.2599 - val_mean_absolute_error: 12.2599\n",
      "Epoch 14/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.4495 - mean_absolute_error: 9.4495\n",
      "Epoch 14: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4003 - mean_absolute_error: 9.4003 - val_loss: 10.2758 - val_mean_absolute_error: 10.2758\n",
      "Epoch 15/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 9.2998 - mean_absolute_error: 9.2998\n",
      "Epoch 15: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4165 - mean_absolute_error: 9.4165 - val_loss: 9.9642 - val_mean_absolute_error: 9.9642\n",
      "Epoch 16/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.4155 - mean_absolute_error: 9.4155\n",
      "Epoch 16: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5238 - mean_absolute_error: 9.5238 - val_loss: 9.9811 - val_mean_absolute_error: 9.9811\n",
      "Epoch 17/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.4897 - mean_absolute_error: 9.4897\n",
      "Epoch 17: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4457 - mean_absolute_error: 9.4457 - val_loss: 10.0973 - val_mean_absolute_error: 10.0973\n",
      "Epoch 18/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.3701 - mean_absolute_error: 9.3701\n",
      "Epoch 18: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3805 - mean_absolute_error: 9.3805 - val_loss: 10.4045 - val_mean_absolute_error: 10.4045\n",
      "Epoch 19/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.2688 - mean_absolute_error: 9.2688\n",
      "Epoch 19: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3688 - mean_absolute_error: 9.3688 - val_loss: 9.9257 - val_mean_absolute_error: 9.9257\n",
      "Epoch 20/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.4613 - mean_absolute_error: 9.4613\n",
      "Epoch 20: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4794 - mean_absolute_error: 9.4794 - val_loss: 10.9357 - val_mean_absolute_error: 10.9357\n",
      "Epoch 21/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.5041 - mean_absolute_error: 9.5041\n",
      "Epoch 21: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4688 - mean_absolute_error: 9.4688 - val_loss: 10.3353 - val_mean_absolute_error: 10.3353\n",
      "Epoch 22/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 9.2944 - mean_absolute_error: 9.2944\n",
      "Epoch 22: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3800 - mean_absolute_error: 9.3800 - val_loss: 10.1002 - val_mean_absolute_error: 10.1002\n",
      "Epoch 23/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.5671 - mean_absolute_error: 9.5671\n",
      "Epoch 23: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5126 - mean_absolute_error: 9.5126 - val_loss: 10.5906 - val_mean_absolute_error: 10.5906\n",
      "Epoch 24/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.4655 - mean_absolute_error: 9.4655\n",
      "Epoch 24: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4815 - mean_absolute_error: 9.4815 - val_loss: 10.0655 - val_mean_absolute_error: 10.0655\n",
      "Epoch 25/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.2678 - mean_absolute_error: 9.2678\n",
      "Epoch 25: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4480 - mean_absolute_error: 9.4480 - val_loss: 11.1947 - val_mean_absolute_error: 11.1947\n",
      "Epoch 26/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1892 - mean_absolute_error: 9.1892\n",
      "Epoch 26: val_loss did not improve from 9.82375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3614 - mean_absolute_error: 9.3614 - val_loss: 10.4983 - val_mean_absolute_error: 10.4983\n",
      "Epoch 27/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.3566 - mean_absolute_error: 9.3566\n",
      "Epoch 27: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3874 - mean_absolute_error: 9.3874 - val_loss: 11.6776 - val_mean_absolute_error: 11.6776\n",
      "Epoch 28/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.4346 - mean_absolute_error: 9.4346\n",
      "Epoch 28: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4779 - mean_absolute_error: 9.4779 - val_loss: 9.9774 - val_mean_absolute_error: 9.9774\n",
      "Epoch 29/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.4031 - mean_absolute_error: 9.4031\n",
      "Epoch 29: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4665 - mean_absolute_error: 9.4665 - val_loss: 10.4273 - val_mean_absolute_error: 10.4273\n",
      "Epoch 30/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.4000 - mean_absolute_error: 9.4000\n",
      "Epoch 30: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3827 - mean_absolute_error: 9.3827 - val_loss: 10.1669 - val_mean_absolute_error: 10.1669\n",
      "Epoch 31/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.4583 - mean_absolute_error: 9.4583\n",
      "Epoch 31: val_loss did not improve from 9.82375\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4953 - mean_absolute_error: 9.4953 - val_loss: 10.1659 - val_mean_absolute_error: 10.1659\n",
      "Epoch 32/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.4630 - mean_absolute_error: 9.4630\n",
      "Epoch 32: val_loss improved from 9.82375 to 9.80418, saving model to Weights-32--9.80418.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3595 - mean_absolute_error: 9.3595 - val_loss: 9.8042 - val_mean_absolute_error: 9.8042\n",
      "Epoch 33/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.4728 - mean_absolute_error: 9.4728\n",
      "Epoch 33: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3899 - mean_absolute_error: 9.3899 - val_loss: 9.9115 - val_mean_absolute_error: 9.9115\n",
      "Epoch 34/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.3732 - mean_absolute_error: 9.3732\n",
      "Epoch 34: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4223 - mean_absolute_error: 9.4223 - val_loss: 9.9289 - val_mean_absolute_error: 9.9289\n",
      "Epoch 35/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.3504 - mean_absolute_error: 9.3504\n",
      "Epoch 35: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3614 - mean_absolute_error: 9.3614 - val_loss: 9.8879 - val_mean_absolute_error: 9.8879\n",
      "Epoch 36/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.4910 - mean_absolute_error: 9.4910\n",
      "Epoch 36: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4211 - mean_absolute_error: 9.4211 - val_loss: 10.2421 - val_mean_absolute_error: 10.2421\n",
      "Epoch 37/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.3250 - mean_absolute_error: 9.3250\n",
      "Epoch 37: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2871 - mean_absolute_error: 9.2871 - val_loss: 9.9873 - val_mean_absolute_error: 9.9873\n",
      "Epoch 38/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.4383 - mean_absolute_error: 9.4383\n",
      "Epoch 38: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3753 - mean_absolute_error: 9.3753 - val_loss: 10.2301 - val_mean_absolute_error: 10.2301\n",
      "Epoch 39/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.2857 - mean_absolute_error: 9.2857\n",
      "Epoch 39: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2900 - mean_absolute_error: 9.2900 - val_loss: 9.9634 - val_mean_absolute_error: 9.9634\n",
      "Epoch 40/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.6211 - mean_absolute_error: 9.6211\n",
      "Epoch 40: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.5877 - mean_absolute_error: 9.5877 - val_loss: 10.0171 - val_mean_absolute_error: 10.0171\n",
      "Epoch 41/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.4411 - mean_absolute_error: 9.4411\n",
      "Epoch 41: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3782 - mean_absolute_error: 9.3782 - val_loss: 9.8238 - val_mean_absolute_error: 9.8238\n",
      "Epoch 42/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.2372 - mean_absolute_error: 9.2372\n",
      "Epoch 42: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2864 - mean_absolute_error: 9.2864 - val_loss: 10.0566 - val_mean_absolute_error: 10.0566\n",
      "Epoch 43/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1984 - mean_absolute_error: 9.1984\n",
      "Epoch 43: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2406 - mean_absolute_error: 9.2406 - val_loss: 10.0402 - val_mean_absolute_error: 10.0402\n",
      "Epoch 44/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.4161 - mean_absolute_error: 9.4161\n",
      "Epoch 44: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3678 - mean_absolute_error: 9.3678 - val_loss: 10.1226 - val_mean_absolute_error: 10.1226\n",
      "Epoch 45/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2729 - mean_absolute_error: 9.2729\n",
      "Epoch 45: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2828 - mean_absolute_error: 9.2828 - val_loss: 10.4483 - val_mean_absolute_error: 10.4483\n",
      "Epoch 46/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.3614 - mean_absolute_error: 9.3614 \n",
      "Epoch 46: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2822 - mean_absolute_error: 9.2822 - val_loss: 10.0269 - val_mean_absolute_error: 10.0269\n",
      "Epoch 47/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.5876 - mean_absolute_error: 9.5876\n",
      "Epoch 47: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4899 - mean_absolute_error: 9.4899 - val_loss: 10.0441 - val_mean_absolute_error: 10.0441\n",
      "Epoch 48/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.2986 - mean_absolute_error: 9.2986\n",
      "Epoch 48: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2506 - mean_absolute_error: 9.2506 - val_loss: 10.3144 - val_mean_absolute_error: 10.3144\n",
      "Epoch 49/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.2823 - mean_absolute_error: 9.2823\n",
      "Epoch 49: val_loss did not improve from 9.80418\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2336 - mean_absolute_error: 9.2336 - val_loss: 10.2106 - val_mean_absolute_error: 10.2106\n",
      "Epoch 50/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 9.3523 - mean_absolute_error: 9.3523\n",
      "Epoch 50: val_loss improved from 9.80418 to 9.79795, saving model to Weights-50--9.79795.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3252 - mean_absolute_error: 9.3252 - val_loss: 9.7979 - val_mean_absolute_error: 9.7979\n",
      "Epoch 51/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.3451 - mean_absolute_error: 9.3451\n",
      "Epoch 51: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2974 - mean_absolute_error: 9.2974 - val_loss: 10.0256 - val_mean_absolute_error: 10.0256\n",
      "Epoch 52/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.1486 - mean_absolute_error: 9.1486\n",
      "Epoch 52: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2078 - mean_absolute_error: 9.2078 - val_loss: 9.8252 - val_mean_absolute_error: 9.8252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.2457 - mean_absolute_error: 9.2457\n",
      "Epoch 53: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1557 - mean_absolute_error: 9.1557 - val_loss: 10.1536 - val_mean_absolute_error: 10.1536\n",
      "Epoch 54/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.1831 - mean_absolute_error: 9.1831\n",
      "Epoch 54: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2379 - mean_absolute_error: 9.2379 - val_loss: 10.0024 - val_mean_absolute_error: 10.0024\n",
      "Epoch 55/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.1497 - mean_absolute_error: 9.1497\n",
      "Epoch 55: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1807 - mean_absolute_error: 9.1807 - val_loss: 9.9972 - val_mean_absolute_error: 9.9972\n",
      "Epoch 56/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.3068 - mean_absolute_error: 9.3068\n",
      "Epoch 56: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2746 - mean_absolute_error: 9.2746 - val_loss: 9.8528 - val_mean_absolute_error: 9.8528\n",
      "Epoch 57/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.3169 - mean_absolute_error: 9.3169\n",
      "Epoch 57: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3253 - mean_absolute_error: 9.3253 - val_loss: 9.9739 - val_mean_absolute_error: 9.9739\n",
      "Epoch 58/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.4184 - mean_absolute_error: 9.4184\n",
      "Epoch 58: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.4263 - mean_absolute_error: 9.4263 - val_loss: 9.9207 - val_mean_absolute_error: 9.9207\n",
      "Epoch 59/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1582 - mean_absolute_error: 9.1582\n",
      "Epoch 59: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1904 - mean_absolute_error: 9.1904 - val_loss: 10.0930 - val_mean_absolute_error: 10.0930\n",
      "Epoch 60/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.2574 - mean_absolute_error: 9.2574\n",
      "Epoch 60: val_loss did not improve from 9.79795\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2128 - mean_absolute_error: 9.2128 - val_loss: 9.8265 - val_mean_absolute_error: 9.8265\n",
      "Epoch 61/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2488 - mean_absolute_error: 9.2488\n",
      "Epoch 61: val_loss improved from 9.79795 to 9.74993, saving model to Weights-61--9.74993.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2072 - mean_absolute_error: 9.2072 - val_loss: 9.7499 - val_mean_absolute_error: 9.7499\n",
      "Epoch 62/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.1259 - mean_absolute_error: 9.1259\n",
      "Epoch 62: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1088 - mean_absolute_error: 9.1088 - val_loss: 10.4003 - val_mean_absolute_error: 10.4003\n",
      "Epoch 63/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2167 - mean_absolute_error: 9.2167\n",
      "Epoch 63: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2320 - mean_absolute_error: 9.2320 - val_loss: 9.7950 - val_mean_absolute_error: 9.7950\n",
      "Epoch 64/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2156 - mean_absolute_error: 9.2156\n",
      "Epoch 64: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1548 - mean_absolute_error: 9.1548 - val_loss: 9.8942 - val_mean_absolute_error: 9.8942\n",
      "Epoch 65/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.2962 - mean_absolute_error: 9.2962\n",
      "Epoch 65: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2870 - mean_absolute_error: 9.2870 - val_loss: 9.9027 - val_mean_absolute_error: 9.9027\n",
      "Epoch 66/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.2685 - mean_absolute_error: 9.2685\n",
      "Epoch 66: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3572 - mean_absolute_error: 9.3572 - val_loss: 9.9281 - val_mean_absolute_error: 9.9281\n",
      "Epoch 67/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.1669 - mean_absolute_error: 9.1669\n",
      "Epoch 67: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2051 - mean_absolute_error: 9.2051 - val_loss: 9.8862 - val_mean_absolute_error: 9.8862\n",
      "Epoch 68/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.2137 - mean_absolute_error: 9.2137\n",
      "Epoch 68: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2166 - mean_absolute_error: 9.2166 - val_loss: 9.9669 - val_mean_absolute_error: 9.9669\n",
      "Epoch 69/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0401 - mean_absolute_error: 9.0401\n",
      "Epoch 69: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2083 - mean_absolute_error: 9.2083 - val_loss: 10.7430 - val_mean_absolute_error: 10.7430\n",
      "Epoch 70/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.3062 - mean_absolute_error: 9.3062\n",
      "Epoch 70: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2356 - mean_absolute_error: 9.2356 - val_loss: 9.9584 - val_mean_absolute_error: 9.9584\n",
      "Epoch 71/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1045 - mean_absolute_error: 9.1045\n",
      "Epoch 71: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1871 - mean_absolute_error: 9.1871 - val_loss: 10.5996 - val_mean_absolute_error: 10.5996\n",
      "Epoch 72/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.3287 - mean_absolute_error: 9.3287\n",
      "Epoch 72: val_loss did not improve from 9.74993\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2650 - mean_absolute_error: 9.2650 - val_loss: 10.5683 - val_mean_absolute_error: 10.5683\n",
      "Epoch 73/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.2090 - mean_absolute_error: 9.2090\n",
      "Epoch 73: val_loss improved from 9.74993 to 9.70097, saving model to Weights-73--9.70097.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2294 - mean_absolute_error: 9.2294 - val_loss: 9.7010 - val_mean_absolute_error: 9.7010\n",
      "Epoch 74/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.2791 - mean_absolute_error: 9.2791\n",
      "Epoch 74: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.3405 - mean_absolute_error: 9.3405 - val_loss: 9.9996 - val_mean_absolute_error: 9.9996\n",
      "Epoch 75/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.2755 - mean_absolute_error: 9.2755\n",
      "Epoch 75: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1928 - mean_absolute_error: 9.1928 - val_loss: 10.0388 - val_mean_absolute_error: 10.0388\n",
      "Epoch 76/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.1154 - mean_absolute_error: 9.1154\n",
      "Epoch 76: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1695 - mean_absolute_error: 9.1695 - val_loss: 9.8466 - val_mean_absolute_error: 9.8466\n",
      "Epoch 77/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1518 - mean_absolute_error: 9.1518\n",
      "Epoch 77: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1371 - mean_absolute_error: 9.1371 - val_loss: 10.1323 - val_mean_absolute_error: 10.1323\n",
      "Epoch 78/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 9.1995 - mean_absolute_error: 9.1995\n",
      "Epoch 78: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1823 - mean_absolute_error: 9.1823 - val_loss: 9.9853 - val_mean_absolute_error: 9.9853\n",
      "Epoch 79/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/136 [========================>.....] - ETA: 0s - loss: 9.2500 - mean_absolute_error: 9.2500\n",
      "Epoch 79: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2133 - mean_absolute_error: 9.2133 - val_loss: 9.7536 - val_mean_absolute_error: 9.7536\n",
      "Epoch 80/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.0978 - mean_absolute_error: 9.0978\n",
      "Epoch 80: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1223 - mean_absolute_error: 9.1223 - val_loss: 9.7641 - val_mean_absolute_error: 9.7641\n",
      "Epoch 81/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.1795 - mean_absolute_error: 9.1795\n",
      "Epoch 81: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1324 - mean_absolute_error: 9.1324 - val_loss: 10.3172 - val_mean_absolute_error: 10.3172\n",
      "Epoch 82/500\n",
      "122/136 [=========================>....] - ETA: 0s - loss: 9.2727 - mean_absolute_error: 9.2727\n",
      "Epoch 82: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2350 - mean_absolute_error: 9.2350 - val_loss: 10.0367 - val_mean_absolute_error: 10.0367\n",
      "Epoch 83/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1942 - mean_absolute_error: 9.1942\n",
      "Epoch 83: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2562 - mean_absolute_error: 9.2562 - val_loss: 9.8802 - val_mean_absolute_error: 9.8802\n",
      "Epoch 84/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2172 - mean_absolute_error: 9.2172\n",
      "Epoch 84: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2136 - mean_absolute_error: 9.2136 - val_loss: 10.2622 - val_mean_absolute_error: 10.2622\n",
      "Epoch 85/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.3357 - mean_absolute_error: 9.3357\n",
      "Epoch 85: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2954 - mean_absolute_error: 9.2954 - val_loss: 10.0529 - val_mean_absolute_error: 10.0529\n",
      "Epoch 86/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.2072 - mean_absolute_error: 9.2072\n",
      "Epoch 86: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1747 - mean_absolute_error: 9.1747 - val_loss: 9.7161 - val_mean_absolute_error: 9.7161\n",
      "Epoch 87/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1604 - mean_absolute_error: 9.1604\n",
      "Epoch 87: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1591 - mean_absolute_error: 9.1591 - val_loss: 10.4416 - val_mean_absolute_error: 10.4416\n",
      "Epoch 88/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1858 - mean_absolute_error: 9.1858\n",
      "Epoch 88: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2002 - mean_absolute_error: 9.2002 - val_loss: 10.1016 - val_mean_absolute_error: 10.1016\n",
      "Epoch 89/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.3620 - mean_absolute_error: 9.3620\n",
      "Epoch 89: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2739 - mean_absolute_error: 9.2739 - val_loss: 10.0849 - val_mean_absolute_error: 10.0849\n",
      "Epoch 90/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.1672 - mean_absolute_error: 9.1672\n",
      "Epoch 90: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1609 - mean_absolute_error: 9.1609 - val_loss: 9.7250 - val_mean_absolute_error: 9.7250\n",
      "Epoch 91/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.0510 - mean_absolute_error: 9.0510\n",
      "Epoch 91: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1725 - mean_absolute_error: 9.1725 - val_loss: 10.2104 - val_mean_absolute_error: 10.2104\n",
      "Epoch 92/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.1306 - mean_absolute_error: 9.1306\n",
      "Epoch 92: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0877 - mean_absolute_error: 9.0877 - val_loss: 9.8734 - val_mean_absolute_error: 9.8734\n",
      "Epoch 93/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0612 - mean_absolute_error: 9.0612\n",
      "Epoch 93: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1069 - mean_absolute_error: 9.1069 - val_loss: 9.9042 - val_mean_absolute_error: 9.9042\n",
      "Epoch 94/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.2857 - mean_absolute_error: 9.2857\n",
      "Epoch 94: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2036 - mean_absolute_error: 9.2036 - val_loss: 9.9352 - val_mean_absolute_error: 9.9352\n",
      "Epoch 95/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.2384 - mean_absolute_error: 9.2384\n",
      "Epoch 95: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1488 - mean_absolute_error: 9.1488 - val_loss: 10.4700 - val_mean_absolute_error: 10.4700\n",
      "Epoch 96/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 9.1934 - mean_absolute_error: 9.1934\n",
      "Epoch 96: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1669 - mean_absolute_error: 9.1669 - val_loss: 10.0509 - val_mean_absolute_error: 10.0509\n",
      "Epoch 97/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.1233 - mean_absolute_error: 9.1233\n",
      "Epoch 97: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1190 - mean_absolute_error: 9.1190 - val_loss: 9.9358 - val_mean_absolute_error: 9.9358\n",
      "Epoch 98/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.0827 - mean_absolute_error: 9.0827\n",
      "Epoch 98: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1119 - mean_absolute_error: 9.1119 - val_loss: 9.9276 - val_mean_absolute_error: 9.9276\n",
      "Epoch 99/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.1292 - mean_absolute_error: 9.1292\n",
      "Epoch 99: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1424 - mean_absolute_error: 9.1424 - val_loss: 10.0077 - val_mean_absolute_error: 10.0077\n",
      "Epoch 100/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.1885 - mean_absolute_error: 9.1885\n",
      "Epoch 100: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.2099 - mean_absolute_error: 9.2099 - val_loss: 9.9635 - val_mean_absolute_error: 9.9635\n",
      "Epoch 101/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.1501 - mean_absolute_error: 9.1501\n",
      "Epoch 101: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0942 - mean_absolute_error: 9.0942 - val_loss: 10.0494 - val_mean_absolute_error: 10.0494\n",
      "Epoch 102/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1655 - mean_absolute_error: 9.1655\n",
      "Epoch 102: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1386 - mean_absolute_error: 9.1386 - val_loss: 10.0706 - val_mean_absolute_error: 10.0706\n",
      "Epoch 103/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0345 - mean_absolute_error: 9.0345\n",
      "Epoch 103: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0537 - mean_absolute_error: 9.0537 - val_loss: 9.8303 - val_mean_absolute_error: 9.8303\n",
      "Epoch 104/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0255 - mean_absolute_error: 9.0255\n",
      "Epoch 104: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1185 - mean_absolute_error: 9.1185 - val_loss: 9.8704 - val_mean_absolute_error: 9.8704\n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/136 [========================>.....] - ETA: 0s - loss: 9.1714 - mean_absolute_error: 9.1714\n",
      "Epoch 105: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1991 - mean_absolute_error: 9.1991 - val_loss: 9.7232 - val_mean_absolute_error: 9.7232\n",
      "Epoch 106/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.1621 - mean_absolute_error: 9.1621\n",
      "Epoch 106: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1279 - mean_absolute_error: 9.1279 - val_loss: 9.8904 - val_mean_absolute_error: 9.8904\n",
      "Epoch 107/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.1615 - mean_absolute_error: 9.1615\n",
      "Epoch 107: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1021 - mean_absolute_error: 9.1021 - val_loss: 9.9701 - val_mean_absolute_error: 9.9701\n",
      "Epoch 108/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.1990 - mean_absolute_error: 9.1990\n",
      "Epoch 108: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1676 - mean_absolute_error: 9.1676 - val_loss: 9.7638 - val_mean_absolute_error: 9.7638\n",
      "Epoch 109/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.1364 - mean_absolute_error: 9.1364\n",
      "Epoch 109: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0981 - mean_absolute_error: 9.0981 - val_loss: 9.8393 - val_mean_absolute_error: 9.8393\n",
      "Epoch 110/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.0654 - mean_absolute_error: 9.0654\n",
      "Epoch 110: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0315 - mean_absolute_error: 9.0315 - val_loss: 9.8547 - val_mean_absolute_error: 9.8547\n",
      "Epoch 111/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.0482 - mean_absolute_error: 9.0482\n",
      "Epoch 111: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1029 - mean_absolute_error: 9.1029 - val_loss: 9.9365 - val_mean_absolute_error: 9.9365\n",
      "Epoch 112/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0329 - mean_absolute_error: 9.0329\n",
      "Epoch 112: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0290 - mean_absolute_error: 9.0290 - val_loss: 9.7228 - val_mean_absolute_error: 9.7228\n",
      "Epoch 113/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.1163 - mean_absolute_error: 9.1163\n",
      "Epoch 113: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0855 - mean_absolute_error: 9.0855 - val_loss: 9.7139 - val_mean_absolute_error: 9.7139\n",
      "Epoch 114/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 9.0236 - mean_absolute_error: 9.0236\n",
      "Epoch 114: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0998 - mean_absolute_error: 9.0998 - val_loss: 9.8553 - val_mean_absolute_error: 9.8553\n",
      "Epoch 115/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9831 - mean_absolute_error: 8.9831\n",
      "Epoch 115: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0500 - mean_absolute_error: 9.0500 - val_loss: 9.9498 - val_mean_absolute_error: 9.9498\n",
      "Epoch 116/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0425 - mean_absolute_error: 9.0425\n",
      "Epoch 116: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0657 - mean_absolute_error: 9.0657 - val_loss: 9.7653 - val_mean_absolute_error: 9.7653\n",
      "Epoch 117/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.0516 - mean_absolute_error: 9.0516\n",
      "Epoch 117: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0357 - mean_absolute_error: 9.0357 - val_loss: 9.7568 - val_mean_absolute_error: 9.7568\n",
      "Epoch 118/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0796 - mean_absolute_error: 9.0796\n",
      "Epoch 118: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0919 - mean_absolute_error: 9.0919 - val_loss: 9.9464 - val_mean_absolute_error: 9.9464\n",
      "Epoch 119/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 9.2377 - mean_absolute_error: 9.2377\n",
      "Epoch 119: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1146 - mean_absolute_error: 9.1146 - val_loss: 9.8220 - val_mean_absolute_error: 9.8220\n",
      "Epoch 120/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.0431 - mean_absolute_error: 9.0431\n",
      "Epoch 120: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0576 - mean_absolute_error: 9.0576 - val_loss: 9.7449 - val_mean_absolute_error: 9.7449\n",
      "Epoch 121/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9801 - mean_absolute_error: 8.9801\n",
      "Epoch 121: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0312 - mean_absolute_error: 9.0312 - val_loss: 9.7792 - val_mean_absolute_error: 9.7792\n",
      "Epoch 122/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.9890 - mean_absolute_error: 8.9890\n",
      "Epoch 122: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0302 - mean_absolute_error: 9.0302 - val_loss: 10.0310 - val_mean_absolute_error: 10.0310\n",
      "Epoch 123/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0934 - mean_absolute_error: 9.0934\n",
      "Epoch 123: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1489 - mean_absolute_error: 9.1489 - val_loss: 10.0780 - val_mean_absolute_error: 10.0780\n",
      "Epoch 124/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.1066 - mean_absolute_error: 9.1066\n",
      "Epoch 124: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0278 - mean_absolute_error: 9.0278 - val_loss: 10.4064 - val_mean_absolute_error: 10.4064\n",
      "Epoch 125/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.1041 - mean_absolute_error: 9.1041\n",
      "Epoch 125: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0470 - mean_absolute_error: 9.0470 - val_loss: 9.9197 - val_mean_absolute_error: 9.9197\n",
      "Epoch 126/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0887 - mean_absolute_error: 9.0887\n",
      "Epoch 126: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0315 - mean_absolute_error: 9.0315 - val_loss: 10.0154 - val_mean_absolute_error: 10.0154\n",
      "Epoch 127/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.1243 - mean_absolute_error: 9.1243\n",
      "Epoch 127: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0617 - mean_absolute_error: 9.0617 - val_loss: 10.2706 - val_mean_absolute_error: 10.2706\n",
      "Epoch 128/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1369 - mean_absolute_error: 9.1369\n",
      "Epoch 128: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0576 - mean_absolute_error: 9.0576 - val_loss: 9.8295 - val_mean_absolute_error: 9.8295\n",
      "Epoch 129/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0894 - mean_absolute_error: 9.0894\n",
      "Epoch 129: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0755 - mean_absolute_error: 9.0755 - val_loss: 9.7574 - val_mean_absolute_error: 9.7574\n",
      "Epoch 130/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.0629 - mean_absolute_error: 9.0629\n",
      "Epoch 130: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0989 - mean_absolute_error: 9.0989 - val_loss: 9.9219 - val_mean_absolute_error: 9.9219\n",
      "Epoch 131/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9446 - mean_absolute_error: 8.9446\n",
      "Epoch 131: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9971 - mean_absolute_error: 8.9971 - val_loss: 9.7030 - val_mean_absolute_error: 9.7030\n",
      "Epoch 132/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.1631 - mean_absolute_error: 9.1631\n",
      "Epoch 132: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1131 - mean_absolute_error: 9.1131 - val_loss: 9.7097 - val_mean_absolute_error: 9.7097\n",
      "Epoch 133/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9771 - mean_absolute_error: 8.9771\n",
      "Epoch 133: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9452 - mean_absolute_error: 8.9452 - val_loss: 9.8432 - val_mean_absolute_error: 9.8432\n",
      "Epoch 134/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9694 - mean_absolute_error: 8.9694\n",
      "Epoch 134: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0105 - mean_absolute_error: 9.0105 - val_loss: 9.9468 - val_mean_absolute_error: 9.9468\n",
      "Epoch 135/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 9.0679 - mean_absolute_error: 9.0679\n",
      "Epoch 135: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0593 - mean_absolute_error: 9.0593 - val_loss: 9.9356 - val_mean_absolute_error: 9.9356\n",
      "Epoch 136/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.1175 - mean_absolute_error: 9.1175\n",
      "Epoch 136: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1158 - mean_absolute_error: 9.1158 - val_loss: 10.0982 - val_mean_absolute_error: 10.0982\n",
      "Epoch 137/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.0501 - mean_absolute_error: 9.0501\n",
      "Epoch 137: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0441 - mean_absolute_error: 9.0441 - val_loss: 9.8764 - val_mean_absolute_error: 9.8764\n",
      "Epoch 138/500\n",
      "122/136 [=========================>....] - ETA: 0s - loss: 8.9878 - mean_absolute_error: 8.9878\n",
      "Epoch 138: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9751 - mean_absolute_error: 8.9751 - val_loss: 9.8935 - val_mean_absolute_error: 9.8935\n",
      "Epoch 139/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9470 - mean_absolute_error: 8.9470\n",
      "Epoch 139: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9986 - mean_absolute_error: 8.9986 - val_loss: 10.0215 - val_mean_absolute_error: 10.0215\n",
      "Epoch 140/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.1117 - mean_absolute_error: 9.1117\n",
      "Epoch 140: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1201 - mean_absolute_error: 9.1201 - val_loss: 10.4715 - val_mean_absolute_error: 10.4715\n",
      "Epoch 141/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 9.0790 - mean_absolute_error: 9.0790\n",
      "Epoch 141: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0348 - mean_absolute_error: 9.0348 - val_loss: 9.7292 - val_mean_absolute_error: 9.7292\n",
      "Epoch 142/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0135 - mean_absolute_error: 9.0135\n",
      "Epoch 142: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0341 - mean_absolute_error: 9.0341 - val_loss: 10.0042 - val_mean_absolute_error: 10.0042\n",
      "Epoch 143/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 9.1694 - mean_absolute_error: 9.1694\n",
      "Epoch 143: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0071 - mean_absolute_error: 9.0071 - val_loss: 9.8118 - val_mean_absolute_error: 9.8118\n",
      "Epoch 144/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 9.1367 - mean_absolute_error: 9.1367\n",
      "Epoch 144: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0916 - mean_absolute_error: 9.0916 - val_loss: 9.8321 - val_mean_absolute_error: 9.8321\n",
      "Epoch 145/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 9.0096 - mean_absolute_error: 9.0096\n",
      "Epoch 145: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0253 - mean_absolute_error: 9.0253 - val_loss: 9.9198 - val_mean_absolute_error: 9.9198\n",
      "Epoch 146/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.0418 - mean_absolute_error: 9.0418\n",
      "Epoch 146: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0369 - mean_absolute_error: 9.0369 - val_loss: 9.8219 - val_mean_absolute_error: 9.8219\n",
      "Epoch 147/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.9687 - mean_absolute_error: 8.9687\n",
      "Epoch 147: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0519 - mean_absolute_error: 9.0519 - val_loss: 9.9339 - val_mean_absolute_error: 9.9339\n",
      "Epoch 148/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9388 - mean_absolute_error: 8.9388\n",
      "Epoch 148: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9908 - mean_absolute_error: 8.9908 - val_loss: 9.7535 - val_mean_absolute_error: 9.7535\n",
      "Epoch 149/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 9.0786 - mean_absolute_error: 9.0786\n",
      "Epoch 149: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9704 - mean_absolute_error: 8.9704 - val_loss: 10.0483 - val_mean_absolute_error: 10.0483\n",
      "Epoch 150/500\n",
      "126/136 [==========================>...] - ETA: 0s - loss: 8.9964 - mean_absolute_error: 8.9964\n",
      "Epoch 150: val_loss did not improve from 9.70097\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0438 - mean_absolute_error: 9.0438 - val_loss: 9.8337 - val_mean_absolute_error: 9.8337\n",
      "Epoch 151/500\n",
      "132/136 [============================>.] - ETA: 0s - loss: 9.0909 - mean_absolute_error: 9.0909\n",
      "Epoch 151: val_loss improved from 9.70097 to 9.66185, saving model to Weights-151--9.66185.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.1005 - mean_absolute_error: 9.1005 - val_loss: 9.6618 - val_mean_absolute_error: 9.6618\n",
      "Epoch 152/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 9.0163 - mean_absolute_error: 9.0163\n",
      "Epoch 152: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0544 - mean_absolute_error: 9.0544 - val_loss: 10.0009 - val_mean_absolute_error: 10.0009\n",
      "Epoch 153/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.9791 - mean_absolute_error: 8.9791\n",
      "Epoch 153: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9349 - mean_absolute_error: 8.9349 - val_loss: 9.9611 - val_mean_absolute_error: 9.9611\n",
      "Epoch 154/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 9.1715 - mean_absolute_error: 9.1715\n",
      "Epoch 154: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0691 - mean_absolute_error: 9.0691 - val_loss: 10.0749 - val_mean_absolute_error: 10.0749\n",
      "Epoch 155/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 9.0581 - mean_absolute_error: 9.0581\n",
      "Epoch 155: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0157 - mean_absolute_error: 9.0157 - val_loss: 9.7266 - val_mean_absolute_error: 9.7266\n",
      "Epoch 156/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 9.1145 - mean_absolute_error: 9.1145\n",
      "Epoch 156: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9998 - mean_absolute_error: 8.9998 - val_loss: 9.8761 - val_mean_absolute_error: 9.8761\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/136 [============================>.] - ETA: 0s - loss: 8.9885 - mean_absolute_error: 8.9885\n",
      "Epoch 157: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0008 - mean_absolute_error: 9.0008 - val_loss: 9.9193 - val_mean_absolute_error: 9.9193\n",
      "Epoch 158/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0857 - mean_absolute_error: 9.0857\n",
      "Epoch 158: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0446 - mean_absolute_error: 9.0446 - val_loss: 10.2612 - val_mean_absolute_error: 10.2612\n",
      "Epoch 159/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9550 - mean_absolute_error: 8.9550\n",
      "Epoch 159: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9819 - mean_absolute_error: 8.9819 - val_loss: 10.1886 - val_mean_absolute_error: 10.1886\n",
      "Epoch 160/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.9470 - mean_absolute_error: 8.9470\n",
      "Epoch 160: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9983 - mean_absolute_error: 8.9983 - val_loss: 9.6667 - val_mean_absolute_error: 9.6667\n",
      "Epoch 161/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.9023 - mean_absolute_error: 8.9023\n",
      "Epoch 161: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9766 - mean_absolute_error: 8.9766 - val_loss: 10.8263 - val_mean_absolute_error: 10.8263\n",
      "Epoch 162/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9383 - mean_absolute_error: 8.9383\n",
      "Epoch 162: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0271 - mean_absolute_error: 9.0271 - val_loss: 10.0854 - val_mean_absolute_error: 10.0854\n",
      "Epoch 163/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9425 - mean_absolute_error: 8.9425\n",
      "Epoch 163: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9604 - mean_absolute_error: 8.9604 - val_loss: 9.7480 - val_mean_absolute_error: 9.7480\n",
      "Epoch 164/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9647 - mean_absolute_error: 8.9647\n",
      "Epoch 164: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9915 - mean_absolute_error: 8.9915 - val_loss: 9.8385 - val_mean_absolute_error: 9.8385\n",
      "Epoch 165/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9985 - mean_absolute_error: 8.9985\n",
      "Epoch 165: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0195 - mean_absolute_error: 9.0195 - val_loss: 9.8222 - val_mean_absolute_error: 9.8222\n",
      "Epoch 166/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9085 - mean_absolute_error: 8.9085\n",
      "Epoch 166: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0089 - mean_absolute_error: 9.0089 - val_loss: 9.8619 - val_mean_absolute_error: 9.8619\n",
      "Epoch 167/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 9.0624 - mean_absolute_error: 9.0624\n",
      "Epoch 167: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0073 - mean_absolute_error: 9.0073 - val_loss: 10.0925 - val_mean_absolute_error: 10.0925\n",
      "Epoch 168/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0268 - mean_absolute_error: 9.0268\n",
      "Epoch 168: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0439 - mean_absolute_error: 9.0439 - val_loss: 9.7764 - val_mean_absolute_error: 9.7764\n",
      "Epoch 169/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 9.0093 - mean_absolute_error: 9.0093\n",
      "Epoch 169: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9997 - mean_absolute_error: 8.9997 - val_loss: 9.6644 - val_mean_absolute_error: 9.6644\n",
      "Epoch 170/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.9778 - mean_absolute_error: 8.9778\n",
      "Epoch 170: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9623 - mean_absolute_error: 8.9623 - val_loss: 9.7737 - val_mean_absolute_error: 9.7737\n",
      "Epoch 171/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9758 - mean_absolute_error: 8.9758\n",
      "Epoch 171: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9594 - mean_absolute_error: 8.9594 - val_loss: 9.9300 - val_mean_absolute_error: 9.9300\n",
      "Epoch 172/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.0322 - mean_absolute_error: 9.0322\n",
      "Epoch 172: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9286 - mean_absolute_error: 8.9286 - val_loss: 9.8168 - val_mean_absolute_error: 9.8168\n",
      "Epoch 173/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9542 - mean_absolute_error: 8.9542\n",
      "Epoch 173: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9632 - mean_absolute_error: 8.9632 - val_loss: 9.8783 - val_mean_absolute_error: 9.8783\n",
      "Epoch 174/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.9396 - mean_absolute_error: 8.9396\n",
      "Epoch 174: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9330 - mean_absolute_error: 8.9330 - val_loss: 9.8472 - val_mean_absolute_error: 9.8472\n",
      "Epoch 175/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.9397 - mean_absolute_error: 8.9397\n",
      "Epoch 175: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0121 - mean_absolute_error: 9.0121 - val_loss: 9.8358 - val_mean_absolute_error: 9.8358\n",
      "Epoch 176/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9367 - mean_absolute_error: 8.9367\n",
      "Epoch 176: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9952 - mean_absolute_error: 8.9952 - val_loss: 9.8981 - val_mean_absolute_error: 9.8981\n",
      "Epoch 177/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0290 - mean_absolute_error: 9.0290\n",
      "Epoch 177: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0117 - mean_absolute_error: 9.0117 - val_loss: 9.8812 - val_mean_absolute_error: 9.8812\n",
      "Epoch 178/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.9658 - mean_absolute_error: 8.9658\n",
      "Epoch 178: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9340 - mean_absolute_error: 8.9340 - val_loss: 9.7952 - val_mean_absolute_error: 9.7952\n",
      "Epoch 179/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0644 - mean_absolute_error: 9.0644\n",
      "Epoch 179: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0367 - mean_absolute_error: 9.0367 - val_loss: 9.7862 - val_mean_absolute_error: 9.7862\n",
      "Epoch 180/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.9833 - mean_absolute_error: 8.9833\n",
      "Epoch 180: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9735 - mean_absolute_error: 8.9735 - val_loss: 9.8301 - val_mean_absolute_error: 9.8301\n",
      "Epoch 181/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9953 - mean_absolute_error: 8.9953\n",
      "Epoch 181: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0176 - mean_absolute_error: 9.0176 - val_loss: 10.0254 - val_mean_absolute_error: 10.0254\n",
      "Epoch 182/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 9.0118 - mean_absolute_error: 9.0118\n",
      "Epoch 182: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9756 - mean_absolute_error: 8.9756 - val_loss: 9.7746 - val_mean_absolute_error: 9.7746\n",
      "Epoch 183/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/136 [========================>.....] - ETA: 0s - loss: 9.0031 - mean_absolute_error: 9.0031\n",
      "Epoch 183: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9932 - mean_absolute_error: 8.9932 - val_loss: 10.0788 - val_mean_absolute_error: 10.0788\n",
      "Epoch 184/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.9443 - mean_absolute_error: 8.9443\n",
      "Epoch 184: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0018 - mean_absolute_error: 9.0018 - val_loss: 9.8064 - val_mean_absolute_error: 9.8064\n",
      "Epoch 185/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9860 - mean_absolute_error: 8.9860\n",
      "Epoch 185: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9682 - mean_absolute_error: 8.9682 - val_loss: 9.7863 - val_mean_absolute_error: 9.7863\n",
      "Epoch 186/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9579 - mean_absolute_error: 8.9579\n",
      "Epoch 186: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9775 - mean_absolute_error: 8.9775 - val_loss: 9.6790 - val_mean_absolute_error: 9.6790\n",
      "Epoch 187/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0386 - mean_absolute_error: 9.0386\n",
      "Epoch 187: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9571 - mean_absolute_error: 8.9571 - val_loss: 9.7708 - val_mean_absolute_error: 9.7708\n",
      "Epoch 188/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.9463 - mean_absolute_error: 8.9463\n",
      "Epoch 188: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9636 - mean_absolute_error: 8.9636 - val_loss: 9.8315 - val_mean_absolute_error: 9.8315\n",
      "Epoch 189/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0592 - mean_absolute_error: 9.0592\n",
      "Epoch 189: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9658 - mean_absolute_error: 8.9658 - val_loss: 9.7524 - val_mean_absolute_error: 9.7524\n",
      "Epoch 190/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9606 - mean_absolute_error: 8.9606\n",
      "Epoch 190: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9749 - mean_absolute_error: 8.9749 - val_loss: 9.6893 - val_mean_absolute_error: 9.6893\n",
      "Epoch 191/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 9.0087 - mean_absolute_error: 9.0087\n",
      "Epoch 191: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0170 - mean_absolute_error: 9.0170 - val_loss: 9.7453 - val_mean_absolute_error: 9.7453\n",
      "Epoch 192/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.8734 - mean_absolute_error: 8.8734\n",
      "Epoch 192: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9245 - mean_absolute_error: 8.9245 - val_loss: 9.8612 - val_mean_absolute_error: 9.8612\n",
      "Epoch 193/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.8658 - mean_absolute_error: 8.8658\n",
      "Epoch 193: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9231 - mean_absolute_error: 8.9231 - val_loss: 10.1720 - val_mean_absolute_error: 10.1720\n",
      "Epoch 194/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 9.0283 - mean_absolute_error: 9.0283\n",
      "Epoch 194: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9490 - mean_absolute_error: 8.9490 - val_loss: 9.8079 - val_mean_absolute_error: 9.8079\n",
      "Epoch 195/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8762 - mean_absolute_error: 8.8762\n",
      "Epoch 195: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9210 - mean_absolute_error: 8.9210 - val_loss: 9.8276 - val_mean_absolute_error: 9.8276\n",
      "Epoch 196/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 9.0505 - mean_absolute_error: 9.0505\n",
      "Epoch 196: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9578 - mean_absolute_error: 8.9578 - val_loss: 9.9421 - val_mean_absolute_error: 9.9421\n",
      "Epoch 197/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.8781 - mean_absolute_error: 8.8781\n",
      "Epoch 197: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9195 - mean_absolute_error: 8.9195 - val_loss: 9.8041 - val_mean_absolute_error: 9.8041\n",
      "Epoch 198/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9340 - mean_absolute_error: 8.9340\n",
      "Epoch 198: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9471 - mean_absolute_error: 8.9471 - val_loss: 9.7399 - val_mean_absolute_error: 9.7399\n",
      "Epoch 199/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.8841 - mean_absolute_error: 8.8841\n",
      "Epoch 199: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9632 - mean_absolute_error: 8.9632 - val_loss: 9.9683 - val_mean_absolute_error: 9.9683\n",
      "Epoch 200/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.8671 - mean_absolute_error: 8.8671\n",
      "Epoch 200: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9218 - mean_absolute_error: 8.9218 - val_loss: 9.7063 - val_mean_absolute_error: 9.7063\n",
      "Epoch 201/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9965 - mean_absolute_error: 8.9965\n",
      "Epoch 201: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9997 - mean_absolute_error: 8.9997 - val_loss: 9.7723 - val_mean_absolute_error: 9.7723\n",
      "Epoch 202/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.8752 - mean_absolute_error: 8.8752\n",
      "Epoch 202: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9704 - mean_absolute_error: 8.9704 - val_loss: 9.7634 - val_mean_absolute_error: 9.7634\n",
      "Epoch 203/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.8952 - mean_absolute_error: 8.8952\n",
      "Epoch 203: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8805 - mean_absolute_error: 8.8805 - val_loss: 9.8496 - val_mean_absolute_error: 9.8496\n",
      "Epoch 204/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.9538 - mean_absolute_error: 8.9538\n",
      "Epoch 204: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9238 - mean_absolute_error: 8.9238 - val_loss: 9.9697 - val_mean_absolute_error: 9.9697\n",
      "Epoch 205/500\n",
      "123/136 [==========================>...] - ETA: 0s - loss: 8.9863 - mean_absolute_error: 8.9863\n",
      "Epoch 205: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9668 - mean_absolute_error: 8.9668 - val_loss: 9.8181 - val_mean_absolute_error: 9.8181\n",
      "Epoch 206/500\n",
      "124/136 [==========================>...] - ETA: 0s - loss: 9.0282 - mean_absolute_error: 9.0282\n",
      "Epoch 206: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9290 - mean_absolute_error: 8.9290 - val_loss: 9.7442 - val_mean_absolute_error: 9.7442\n",
      "Epoch 207/500\n",
      "131/136 [===========================>..] - ETA: 0s - loss: 8.9053 - mean_absolute_error: 8.9053\n",
      "Epoch 207: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9131 - mean_absolute_error: 8.9131 - val_loss: 9.6843 - val_mean_absolute_error: 9.6843\n",
      "Epoch 208/500\n",
      "101/136 [=====================>........] - ETA: 0s - loss: 8.9150 - mean_absolute_error: 8.9150\n",
      "Epoch 208: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9236 - mean_absolute_error: 8.9236 - val_loss: 10.1215 - val_mean_absolute_error: 10.1215\n",
      "Epoch 209/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/136 [======================>.......] - ETA: 0s - loss: 8.8494 - mean_absolute_error: 8.8494\n",
      "Epoch 209: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9166 - mean_absolute_error: 8.9166 - val_loss: 10.1944 - val_mean_absolute_error: 10.1944\n",
      "Epoch 210/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 9.1891 - mean_absolute_error: 9.1891\n",
      "Epoch 210: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 9.0013 - mean_absolute_error: 9.0013 - val_loss: 9.7371 - val_mean_absolute_error: 9.7371\n",
      "Epoch 211/500\n",
      "136/136 [==============================] - ETA: 0s - loss: 8.9142 - mean_absolute_error: 8.9142\n",
      "Epoch 211: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9142 - mean_absolute_error: 8.9142 - val_loss: 9.7817 - val_mean_absolute_error: 9.7817\n",
      "Epoch 212/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.9017 - mean_absolute_error: 8.9017\n",
      "Epoch 212: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9089 - mean_absolute_error: 8.9089 - val_loss: 9.8312 - val_mean_absolute_error: 9.8312\n",
      "Epoch 213/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8912 - mean_absolute_error: 8.8912\n",
      "Epoch 213: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8776 - mean_absolute_error: 8.8776 - val_loss: 9.7952 - val_mean_absolute_error: 9.7952\n",
      "Epoch 214/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.8237 - mean_absolute_error: 8.8237\n",
      "Epoch 214: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9442 - mean_absolute_error: 8.9442 - val_loss: 9.9017 - val_mean_absolute_error: 9.9017\n",
      "Epoch 215/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.8981 - mean_absolute_error: 8.8981\n",
      "Epoch 215: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9707 - mean_absolute_error: 8.9707 - val_loss: 9.7662 - val_mean_absolute_error: 9.7662\n",
      "Epoch 216/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9507 - mean_absolute_error: 8.9507\n",
      "Epoch 216: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9030 - mean_absolute_error: 8.9030 - val_loss: 9.7612 - val_mean_absolute_error: 9.7612\n",
      "Epoch 217/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.8714 - mean_absolute_error: 8.8714\n",
      "Epoch 217: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8977 - mean_absolute_error: 8.8977 - val_loss: 10.0278 - val_mean_absolute_error: 10.0278\n",
      "Epoch 218/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.9926 - mean_absolute_error: 8.9926\n",
      "Epoch 218: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9559 - mean_absolute_error: 8.9559 - val_loss: 9.9741 - val_mean_absolute_error: 9.9741\n",
      "Epoch 219/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 9.0168 - mean_absolute_error: 9.0168\n",
      "Epoch 219: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9504 - mean_absolute_error: 8.9504 - val_loss: 9.8321 - val_mean_absolute_error: 9.8321\n",
      "Epoch 220/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 9.0715 - mean_absolute_error: 9.0715\n",
      "Epoch 220: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9899 - mean_absolute_error: 8.9899 - val_loss: 9.7193 - val_mean_absolute_error: 9.7193\n",
      "Epoch 221/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.8847 - mean_absolute_error: 8.8847\n",
      "Epoch 221: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9271 - mean_absolute_error: 8.9271 - val_loss: 9.7403 - val_mean_absolute_error: 9.7403\n",
      "Epoch 222/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.9518 - mean_absolute_error: 8.9518\n",
      "Epoch 222: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9634 - mean_absolute_error: 8.9634 - val_loss: 9.7299 - val_mean_absolute_error: 9.7299\n",
      "Epoch 223/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.8426 - mean_absolute_error: 8.8426\n",
      "Epoch 223: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8496 - mean_absolute_error: 8.8496 - val_loss: 9.9717 - val_mean_absolute_error: 9.9717\n",
      "Epoch 224/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.9974 - mean_absolute_error: 8.9974\n",
      "Epoch 224: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9945 - mean_absolute_error: 8.9945 - val_loss: 9.8183 - val_mean_absolute_error: 9.8183\n",
      "Epoch 225/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.9493 - mean_absolute_error: 8.9493\n",
      "Epoch 225: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9665 - mean_absolute_error: 8.9665 - val_loss: 10.0964 - val_mean_absolute_error: 10.0964\n",
      "Epoch 226/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.9818 - mean_absolute_error: 8.9818\n",
      "Epoch 226: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8764 - mean_absolute_error: 8.8764 - val_loss: 10.0601 - val_mean_absolute_error: 10.0601\n",
      "Epoch 227/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9338 - mean_absolute_error: 8.9338\n",
      "Epoch 227: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9470 - mean_absolute_error: 8.9470 - val_loss: 9.9915 - val_mean_absolute_error: 9.9915\n",
      "Epoch 228/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.9267 - mean_absolute_error: 8.9267\n",
      "Epoch 228: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9017 - mean_absolute_error: 8.9017 - val_loss: 9.8885 - val_mean_absolute_error: 9.8885\n",
      "Epoch 229/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.7031 - mean_absolute_error: 8.7031\n",
      "Epoch 229: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8668 - mean_absolute_error: 8.8668 - val_loss: 10.2615 - val_mean_absolute_error: 10.2615\n",
      "Epoch 230/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8977 - mean_absolute_error: 8.8977\n",
      "Epoch 230: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9228 - mean_absolute_error: 8.9228 - val_loss: 9.9416 - val_mean_absolute_error: 9.9416\n",
      "Epoch 231/500\n",
      "132/136 [============================>.] - ETA: 0s - loss: 8.8615 - mean_absolute_error: 8.8615\n",
      "Epoch 231: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8798 - mean_absolute_error: 8.8798 - val_loss: 9.8256 - val_mean_absolute_error: 9.8256\n",
      "Epoch 232/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 9.0190 - mean_absolute_error: 9.0190\n",
      "Epoch 232: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9219 - mean_absolute_error: 8.9219 - val_loss: 9.7598 - val_mean_absolute_error: 9.7598\n",
      "Epoch 233/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.9269 - mean_absolute_error: 8.9269\n",
      "Epoch 233: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8850 - mean_absolute_error: 8.8850 - val_loss: 9.9914 - val_mean_absolute_error: 9.9914\n",
      "Epoch 234/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 8.8394 - mean_absolute_error: 8.8394\n",
      "Epoch 234: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8822 - mean_absolute_error: 8.8822 - val_loss: 10.1000 - val_mean_absolute_error: 10.1000\n",
      "Epoch 235/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/136 [=======================>......] - ETA: 0s - loss: 8.9177 - mean_absolute_error: 8.9177\n",
      "Epoch 235: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9130 - mean_absolute_error: 8.9130 - val_loss: 9.8762 - val_mean_absolute_error: 9.8762\n",
      "Epoch 236/500\n",
      "125/136 [==========================>...] - ETA: 0s - loss: 8.9172 - mean_absolute_error: 8.9172\n",
      "Epoch 236: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9304 - mean_absolute_error: 8.9304 - val_loss: 9.7997 - val_mean_absolute_error: 9.7997\n",
      "Epoch 237/500\n",
      "130/136 [===========================>..] - ETA: 0s - loss: 8.8869 - mean_absolute_error: 8.8869\n",
      "Epoch 237: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8608 - mean_absolute_error: 8.8608 - val_loss: 9.8224 - val_mean_absolute_error: 9.8224\n",
      "Epoch 238/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.9377 - mean_absolute_error: 8.9377\n",
      "Epoch 238: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8693 - mean_absolute_error: 8.8693 - val_loss: 10.1543 - val_mean_absolute_error: 10.1543\n",
      "Epoch 239/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8701 - mean_absolute_error: 8.8701\n",
      "Epoch 239: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9156 - mean_absolute_error: 8.9156 - val_loss: 9.7825 - val_mean_absolute_error: 9.7825\n",
      "Epoch 240/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9237 - mean_absolute_error: 8.9237\n",
      "Epoch 240: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9400 - mean_absolute_error: 8.9400 - val_loss: 9.7650 - val_mean_absolute_error: 9.7650\n",
      "Epoch 241/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.9406 - mean_absolute_error: 8.9406\n",
      "Epoch 241: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8920 - mean_absolute_error: 8.8920 - val_loss: 9.7043 - val_mean_absolute_error: 9.7043\n",
      "Epoch 242/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.7617 - mean_absolute_error: 8.7617\n",
      "Epoch 242: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8603 - mean_absolute_error: 8.8603 - val_loss: 9.7205 - val_mean_absolute_error: 9.7205\n",
      "Epoch 243/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.9849 - mean_absolute_error: 8.9849\n",
      "Epoch 243: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9871 - mean_absolute_error: 8.9871 - val_loss: 9.7614 - val_mean_absolute_error: 9.7614\n",
      "Epoch 244/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9002 - mean_absolute_error: 8.9002\n",
      "Epoch 244: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8915 - mean_absolute_error: 8.8915 - val_loss: 9.8212 - val_mean_absolute_error: 9.8212\n",
      "Epoch 245/500\n",
      "128/136 [===========================>..] - ETA: 0s - loss: 8.8367 - mean_absolute_error: 8.8367\n",
      "Epoch 245: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8639 - mean_absolute_error: 8.8639 - val_loss: 9.8194 - val_mean_absolute_error: 9.8194\n",
      "Epoch 246/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.8795 - mean_absolute_error: 8.8795\n",
      "Epoch 246: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8770 - mean_absolute_error: 8.8770 - val_loss: 10.0525 - val_mean_absolute_error: 10.0525\n",
      "Epoch 247/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.9306 - mean_absolute_error: 8.9306\n",
      "Epoch 247: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8890 - mean_absolute_error: 8.8890 - val_loss: 10.0227 - val_mean_absolute_error: 10.0227\n",
      "Epoch 248/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8710 - mean_absolute_error: 8.8710\n",
      "Epoch 248: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9036 - mean_absolute_error: 8.9036 - val_loss: 9.7023 - val_mean_absolute_error: 9.7023\n",
      "Epoch 249/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.7902 - mean_absolute_error: 8.7902\n",
      "Epoch 249: val_loss did not improve from 9.66185\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8487 - mean_absolute_error: 8.8487 - val_loss: 10.0320 - val_mean_absolute_error: 10.0320\n",
      "Epoch 250/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.9209 - mean_absolute_error: 8.9209\n",
      "Epoch 250: val_loss improved from 9.66185 to 9.66166, saving model to Weights-250--9.66166.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9153 - mean_absolute_error: 8.9153 - val_loss: 9.6617 - val_mean_absolute_error: 9.6617\n",
      "Epoch 251/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8942 - mean_absolute_error: 8.8942\n",
      "Epoch 251: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8787 - mean_absolute_error: 8.8787 - val_loss: 9.8245 - val_mean_absolute_error: 9.8245\n",
      "Epoch 252/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.9548 - mean_absolute_error: 8.9548\n",
      "Epoch 252: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9110 - mean_absolute_error: 8.9110 - val_loss: 9.7222 - val_mean_absolute_error: 9.7222\n",
      "Epoch 253/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.8167 - mean_absolute_error: 8.8167\n",
      "Epoch 253: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8681 - mean_absolute_error: 8.8681 - val_loss: 9.7690 - val_mean_absolute_error: 9.7690\n",
      "Epoch 254/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.8298 - mean_absolute_error: 8.8298\n",
      "Epoch 254: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8561 - mean_absolute_error: 8.8561 - val_loss: 9.8865 - val_mean_absolute_error: 9.8865\n",
      "Epoch 255/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.9103 - mean_absolute_error: 8.9103\n",
      "Epoch 255: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8670 - mean_absolute_error: 8.8670 - val_loss: 9.8897 - val_mean_absolute_error: 9.8897\n",
      "Epoch 256/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.8089 - mean_absolute_error: 8.8089\n",
      "Epoch 256: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8476 - mean_absolute_error: 8.8476 - val_loss: 9.7362 - val_mean_absolute_error: 9.7362\n",
      "Epoch 257/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.8841 - mean_absolute_error: 8.8841\n",
      "Epoch 257: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8784 - mean_absolute_error: 8.8784 - val_loss: 9.8694 - val_mean_absolute_error: 9.8694\n",
      "Epoch 258/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.9466 - mean_absolute_error: 8.9466\n",
      "Epoch 258: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9546 - mean_absolute_error: 8.9546 - val_loss: 9.8412 - val_mean_absolute_error: 9.8412\n",
      "Epoch 259/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.8189 - mean_absolute_error: 8.8189\n",
      "Epoch 259: val_loss did not improve from 9.66166\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8574 - mean_absolute_error: 8.8574 - val_loss: 9.9244 - val_mean_absolute_error: 9.9244\n",
      "Epoch 260/500\n",
      "132/136 [============================>.] - ETA: 0s - loss: 8.7672 - mean_absolute_error: 8.7672\n",
      "Epoch 260: val_loss improved from 9.66166 to 9.60305, saving model to Weights-260--9.60305.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8007 - mean_absolute_error: 8.8007 - val_loss: 9.6031 - val_mean_absolute_error: 9.6031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261/500\n",
      "133/136 [============================>.] - ETA: 0s - loss: 8.8746 - mean_absolute_error: 8.8746\n",
      "Epoch 261: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8594 - mean_absolute_error: 8.8594 - val_loss: 9.6685 - val_mean_absolute_error: 9.6685\n",
      "Epoch 262/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9151 - mean_absolute_error: 8.9151\n",
      "Epoch 262: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8289 - mean_absolute_error: 8.8289 - val_loss: 9.6737 - val_mean_absolute_error: 9.6737\n",
      "Epoch 263/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9224 - mean_absolute_error: 8.9224\n",
      "Epoch 263: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8285 - mean_absolute_error: 8.8285 - val_loss: 9.6667 - val_mean_absolute_error: 9.6667\n",
      "Epoch 264/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.8286 - mean_absolute_error: 8.8286\n",
      "Epoch 264: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8524 - mean_absolute_error: 8.8524 - val_loss: 9.8595 - val_mean_absolute_error: 9.8595\n",
      "Epoch 265/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.8466 - mean_absolute_error: 8.8466\n",
      "Epoch 265: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8174 - mean_absolute_error: 8.8174 - val_loss: 9.8296 - val_mean_absolute_error: 9.8296\n",
      "Epoch 266/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.8642 - mean_absolute_error: 8.8642\n",
      "Epoch 266: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8768 - mean_absolute_error: 8.8768 - val_loss: 10.3143 - val_mean_absolute_error: 10.3143\n",
      "Epoch 267/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.8254 - mean_absolute_error: 8.8254\n",
      "Epoch 267: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8683 - mean_absolute_error: 8.8683 - val_loss: 9.6970 - val_mean_absolute_error: 9.6970\n",
      "Epoch 268/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.8551 - mean_absolute_error: 8.8551\n",
      "Epoch 268: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9098 - mean_absolute_error: 8.9098 - val_loss: 9.9090 - val_mean_absolute_error: 9.9090\n",
      "Epoch 269/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8975 - mean_absolute_error: 8.8975\n",
      "Epoch 269: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8486 - mean_absolute_error: 8.8486 - val_loss: 9.8689 - val_mean_absolute_error: 9.8689\n",
      "Epoch 270/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.9375 - mean_absolute_error: 8.9375\n",
      "Epoch 270: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8551 - mean_absolute_error: 8.8551 - val_loss: 10.0006 - val_mean_absolute_error: 10.0006\n",
      "Epoch 271/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 8.8945 - mean_absolute_error: 8.8945\n",
      "Epoch 271: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8247 - mean_absolute_error: 8.8247 - val_loss: 9.8175 - val_mean_absolute_error: 9.8175\n",
      "Epoch 272/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.8598 - mean_absolute_error: 8.8598\n",
      "Epoch 272: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8191 - mean_absolute_error: 8.8191 - val_loss: 9.7515 - val_mean_absolute_error: 9.7515\n",
      "Epoch 273/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.8589 - mean_absolute_error: 8.8589\n",
      "Epoch 273: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8385 - mean_absolute_error: 8.8385 - val_loss: 9.7783 - val_mean_absolute_error: 9.7783\n",
      "Epoch 274/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.9102 - mean_absolute_error: 8.9102\n",
      "Epoch 274: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9158 - mean_absolute_error: 8.9158 - val_loss: 9.7851 - val_mean_absolute_error: 9.7851\n",
      "Epoch 275/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.8448 - mean_absolute_error: 8.8448\n",
      "Epoch 275: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8038 - mean_absolute_error: 8.8038 - val_loss: 9.9981 - val_mean_absolute_error: 9.9981\n",
      "Epoch 276/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.8770 - mean_absolute_error: 8.8770\n",
      "Epoch 276: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8189 - mean_absolute_error: 8.8189 - val_loss: 9.7240 - val_mean_absolute_error: 9.7240\n",
      "Epoch 277/500\n",
      "134/136 [============================>.] - ETA: 0s - loss: 8.7972 - mean_absolute_error: 8.7972\n",
      "Epoch 277: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8093 - mean_absolute_error: 8.8093 - val_loss: 9.7253 - val_mean_absolute_error: 9.7253\n",
      "Epoch 278/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.8868 - mean_absolute_error: 8.8868\n",
      "Epoch 278: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8833 - mean_absolute_error: 8.8833 - val_loss: 9.7609 - val_mean_absolute_error: 9.7609\n",
      "Epoch 279/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 8.7618 - mean_absolute_error: 8.7618\n",
      "Epoch 279: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8244 - mean_absolute_error: 8.8244 - val_loss: 10.3992 - val_mean_absolute_error: 10.3992\n",
      "Epoch 280/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8213 - mean_absolute_error: 8.8213\n",
      "Epoch 280: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8002 - mean_absolute_error: 8.8002 - val_loss: 9.7371 - val_mean_absolute_error: 9.7371\n",
      "Epoch 281/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.7934 - mean_absolute_error: 8.7934\n",
      "Epoch 281: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8132 - mean_absolute_error: 8.8132 - val_loss: 9.7998 - val_mean_absolute_error: 9.7998\n",
      "Epoch 282/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.9087 - mean_absolute_error: 8.9087\n",
      "Epoch 282: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8821 - mean_absolute_error: 8.8821 - val_loss: 9.7539 - val_mean_absolute_error: 9.7539\n",
      "Epoch 283/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.9288 - mean_absolute_error: 8.9288\n",
      "Epoch 283: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8350 - mean_absolute_error: 8.8350 - val_loss: 9.7106 - val_mean_absolute_error: 9.7106\n",
      "Epoch 284/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.7626 - mean_absolute_error: 8.7626\n",
      "Epoch 284: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7924 - mean_absolute_error: 8.7924 - val_loss: 9.8589 - val_mean_absolute_error: 9.8589\n",
      "Epoch 285/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 8.8145 - mean_absolute_error: 8.8145\n",
      "Epoch 285: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7921 - mean_absolute_error: 8.7921 - val_loss: 9.6936 - val_mean_absolute_error: 9.6936\n",
      "Epoch 286/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.9736 - mean_absolute_error: 8.9736\n",
      "Epoch 286: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8715 - mean_absolute_error: 8.8715 - val_loss: 9.9322 - val_mean_absolute_error: 9.9322\n",
      "Epoch 287/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/136 [=======================>......] - ETA: 0s - loss: 8.8083 - mean_absolute_error: 8.8083\n",
      "Epoch 287: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8096 - mean_absolute_error: 8.8096 - val_loss: 9.7984 - val_mean_absolute_error: 9.7984\n",
      "Epoch 288/500\n",
      "130/136 [===========================>..] - ETA: 0s - loss: 8.7711 - mean_absolute_error: 8.7711\n",
      "Epoch 288: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8128 - mean_absolute_error: 8.8128 - val_loss: 9.8198 - val_mean_absolute_error: 9.8198\n",
      "Epoch 289/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8083 - mean_absolute_error: 8.8083\n",
      "Epoch 289: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7655 - mean_absolute_error: 8.7655 - val_loss: 9.6823 - val_mean_absolute_error: 9.6823\n",
      "Epoch 290/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6303 - mean_absolute_error: 8.6303\n",
      "Epoch 290: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8030 - mean_absolute_error: 8.8030 - val_loss: 9.6661 - val_mean_absolute_error: 9.6661\n",
      "Epoch 291/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.7679 - mean_absolute_error: 8.7679\n",
      "Epoch 291: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7887 - mean_absolute_error: 8.7887 - val_loss: 9.7472 - val_mean_absolute_error: 9.7472\n",
      "Epoch 292/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7190 - mean_absolute_error: 8.7190\n",
      "Epoch 292: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8034 - mean_absolute_error: 8.8034 - val_loss: 9.6547 - val_mean_absolute_error: 9.6547\n",
      "Epoch 293/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.8220 - mean_absolute_error: 8.8220\n",
      "Epoch 293: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8151 - mean_absolute_error: 8.8151 - val_loss: 9.7378 - val_mean_absolute_error: 9.7378\n",
      "Epoch 294/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.8945 - mean_absolute_error: 8.8945\n",
      "Epoch 294: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8071 - mean_absolute_error: 8.8071 - val_loss: 10.1051 - val_mean_absolute_error: 10.1051\n",
      "Epoch 295/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.9376 - mean_absolute_error: 8.9376\n",
      "Epoch 295: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.9164 - mean_absolute_error: 8.9164 - val_loss: 9.8698 - val_mean_absolute_error: 9.8698\n",
      "Epoch 296/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8636 - mean_absolute_error: 8.8636\n",
      "Epoch 296: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8414 - mean_absolute_error: 8.8414 - val_loss: 9.7182 - val_mean_absolute_error: 9.7182\n",
      "Epoch 297/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.7365 - mean_absolute_error: 8.7365\n",
      "Epoch 297: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8485 - mean_absolute_error: 8.8485 - val_loss: 9.7872 - val_mean_absolute_error: 9.7872\n",
      "Epoch 298/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8777 - mean_absolute_error: 8.8777\n",
      "Epoch 298: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8082 - mean_absolute_error: 8.8082 - val_loss: 9.6404 - val_mean_absolute_error: 9.6404\n",
      "Epoch 299/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.7126 - mean_absolute_error: 8.7126\n",
      "Epoch 299: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7592 - mean_absolute_error: 8.7592 - val_loss: 9.8232 - val_mean_absolute_error: 9.8232\n",
      "Epoch 300/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8410 - mean_absolute_error: 8.8410\n",
      "Epoch 300: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7328 - mean_absolute_error: 8.7328 - val_loss: 9.7733 - val_mean_absolute_error: 9.7733\n",
      "Epoch 301/500\n",
      "127/136 [===========================>..] - ETA: 0s - loss: 8.8137 - mean_absolute_error: 8.8137\n",
      "Epoch 301: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8557 - mean_absolute_error: 8.8557 - val_loss: 10.0099 - val_mean_absolute_error: 10.0099\n",
      "Epoch 302/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.8283 - mean_absolute_error: 8.8283\n",
      "Epoch 302: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8336 - mean_absolute_error: 8.8336 - val_loss: 9.8067 - val_mean_absolute_error: 9.8067\n",
      "Epoch 303/500\n",
      "129/136 [===========================>..] - ETA: 0s - loss: 8.7833 - mean_absolute_error: 8.7833\n",
      "Epoch 303: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7713 - mean_absolute_error: 8.7713 - val_loss: 9.8172 - val_mean_absolute_error: 9.8172\n",
      "Epoch 304/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.8109 - mean_absolute_error: 8.8109\n",
      "Epoch 304: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8204 - mean_absolute_error: 8.8204 - val_loss: 9.6626 - val_mean_absolute_error: 9.6626\n",
      "Epoch 305/500\n",
      "122/136 [=========================>....] - ETA: 0s - loss: 8.7805 - mean_absolute_error: 8.7805\n",
      "Epoch 305: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7984 - mean_absolute_error: 8.7984 - val_loss: 9.8823 - val_mean_absolute_error: 9.8823\n",
      "Epoch 306/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.9671 - mean_absolute_error: 8.9671\n",
      "Epoch 306: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8856 - mean_absolute_error: 8.8856 - val_loss: 9.8243 - val_mean_absolute_error: 9.8243\n",
      "Epoch 307/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.7494 - mean_absolute_error: 8.7494\n",
      "Epoch 307: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8063 - mean_absolute_error: 8.8063 - val_loss: 9.8532 - val_mean_absolute_error: 9.8532\n",
      "Epoch 308/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.8052 - mean_absolute_error: 8.8052\n",
      "Epoch 308: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8315 - mean_absolute_error: 8.8315 - val_loss: 9.6605 - val_mean_absolute_error: 9.6605\n",
      "Epoch 309/500\n",
      "123/136 [==========================>...] - ETA: 0s - loss: 8.7432 - mean_absolute_error: 8.7432\n",
      "Epoch 309: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7909 - mean_absolute_error: 8.7909 - val_loss: 9.6232 - val_mean_absolute_error: 9.6232\n",
      "Epoch 310/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.8321 - mean_absolute_error: 8.8321\n",
      "Epoch 310: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8306 - mean_absolute_error: 8.8306 - val_loss: 9.9409 - val_mean_absolute_error: 9.9409\n",
      "Epoch 311/500\n",
      "122/136 [=========================>....] - ETA: 0s - loss: 8.8194 - mean_absolute_error: 8.8194\n",
      "Epoch 311: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8201 - mean_absolute_error: 8.8201 - val_loss: 9.6762 - val_mean_absolute_error: 9.6762\n",
      "Epoch 312/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.7518 - mean_absolute_error: 8.7518\n",
      "Epoch 312: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7888 - mean_absolute_error: 8.7888 - val_loss: 9.9344 - val_mean_absolute_error: 9.9344\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/136 [============================>.] - ETA: 0s - loss: 8.8322 - mean_absolute_error: 8.8322\n",
      "Epoch 313: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8236 - mean_absolute_error: 8.8236 - val_loss: 9.6689 - val_mean_absolute_error: 9.6689\n",
      "Epoch 314/500\n",
      "130/136 [===========================>..] - ETA: 0s - loss: 8.7799 - mean_absolute_error: 8.7799\n",
      "Epoch 314: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7810 - mean_absolute_error: 8.7810 - val_loss: 9.7146 - val_mean_absolute_error: 9.7146\n",
      "Epoch 315/500\n",
      "128/136 [===========================>..] - ETA: 0s - loss: 8.7685 - mean_absolute_error: 8.7685\n",
      "Epoch 315: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7624 - mean_absolute_error: 8.7624 - val_loss: 10.0379 - val_mean_absolute_error: 10.0379\n",
      "Epoch 316/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 8.6486 - mean_absolute_error: 8.6486\n",
      "Epoch 316: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7825 - mean_absolute_error: 8.7825 - val_loss: 9.7628 - val_mean_absolute_error: 9.7628\n",
      "Epoch 317/500\n",
      "131/136 [===========================>..] - ETA: 0s - loss: 8.8143 - mean_absolute_error: 8.8143\n",
      "Epoch 317: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7995 - mean_absolute_error: 8.7995 - val_loss: 9.8356 - val_mean_absolute_error: 9.8356\n",
      "Epoch 318/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.7702 - mean_absolute_error: 8.7702\n",
      "Epoch 318: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7653 - mean_absolute_error: 8.7653 - val_loss: 9.6622 - val_mean_absolute_error: 9.6622\n",
      "Epoch 319/500\n",
      "132/136 [============================>.] - ETA: 0s - loss: 8.7545 - mean_absolute_error: 8.7545\n",
      "Epoch 319: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7461 - mean_absolute_error: 8.7461 - val_loss: 9.7141 - val_mean_absolute_error: 9.7141\n",
      "Epoch 320/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.8822 - mean_absolute_error: 8.8822\n",
      "Epoch 320: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8797 - mean_absolute_error: 8.8797 - val_loss: 9.7890 - val_mean_absolute_error: 9.7890\n",
      "Epoch 321/500\n",
      "133/136 [============================>.] - ETA: 0s - loss: 8.8287 - mean_absolute_error: 8.8287\n",
      "Epoch 321: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7892 - mean_absolute_error: 8.7892 - val_loss: 9.6290 - val_mean_absolute_error: 9.6290\n",
      "Epoch 322/500\n",
      "122/136 [=========================>....] - ETA: 0s - loss: 8.8688 - mean_absolute_error: 8.8688\n",
      "Epoch 322: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8187 - mean_absolute_error: 8.8187 - val_loss: 9.8299 - val_mean_absolute_error: 9.8299\n",
      "Epoch 323/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7333 - mean_absolute_error: 8.7333\n",
      "Epoch 323: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7768 - mean_absolute_error: 8.7768 - val_loss: 9.6393 - val_mean_absolute_error: 9.6393\n",
      "Epoch 324/500\n",
      "130/136 [===========================>..] - ETA: 0s - loss: 8.7292 - mean_absolute_error: 8.7292\n",
      "Epoch 324: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7248 - mean_absolute_error: 8.7248 - val_loss: 9.9132 - val_mean_absolute_error: 9.9132\n",
      "Epoch 325/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7424 - mean_absolute_error: 8.7424\n",
      "Epoch 325: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7535 - mean_absolute_error: 8.7535 - val_loss: 9.7609 - val_mean_absolute_error: 9.7609\n",
      "Epoch 326/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.7193 - mean_absolute_error: 8.7193\n",
      "Epoch 326: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7856 - mean_absolute_error: 8.7856 - val_loss: 9.8305 - val_mean_absolute_error: 9.8305\n",
      "Epoch 327/500\n",
      "136/136 [==============================] - ETA: 0s - loss: 8.7435 - mean_absolute_error: 8.7435\n",
      "Epoch 327: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7435 - mean_absolute_error: 8.7435 - val_loss: 9.9634 - val_mean_absolute_error: 9.9634\n",
      "Epoch 328/500\n",
      "130/136 [===========================>..] - ETA: 0s - loss: 8.7354 - mean_absolute_error: 8.7354\n",
      "Epoch 328: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7852 - mean_absolute_error: 8.7852 - val_loss: 9.7227 - val_mean_absolute_error: 9.7227\n",
      "Epoch 329/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6931 - mean_absolute_error: 8.6931\n",
      "Epoch 329: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7664 - mean_absolute_error: 8.7664 - val_loss: 9.7470 - val_mean_absolute_error: 9.7470\n",
      "Epoch 330/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.6855 - mean_absolute_error: 8.6855\n",
      "Epoch 330: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7640 - mean_absolute_error: 8.7640 - val_loss: 9.6898 - val_mean_absolute_error: 9.6898\n",
      "Epoch 331/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.8073 - mean_absolute_error: 8.8073\n",
      "Epoch 331: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7820 - mean_absolute_error: 8.7820 - val_loss: 9.7763 - val_mean_absolute_error: 9.7763\n",
      "Epoch 332/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7568 - mean_absolute_error: 8.7568\n",
      "Epoch 332: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7316 - mean_absolute_error: 8.7316 - val_loss: 9.7382 - val_mean_absolute_error: 9.7382\n",
      "Epoch 333/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7960 - mean_absolute_error: 8.7960\n",
      "Epoch 333: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8105 - mean_absolute_error: 8.8105 - val_loss: 9.7962 - val_mean_absolute_error: 9.7962\n",
      "Epoch 334/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.8651 - mean_absolute_error: 8.8651\n",
      "Epoch 334: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.8555 - mean_absolute_error: 8.8555 - val_loss: 9.9146 - val_mean_absolute_error: 9.9146\n",
      "Epoch 335/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6485 - mean_absolute_error: 8.6485\n",
      "Epoch 335: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7348 - mean_absolute_error: 8.7348 - val_loss: 9.6645 - val_mean_absolute_error: 9.6645\n",
      "Epoch 336/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7883 - mean_absolute_error: 8.7883\n",
      "Epoch 336: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7452 - mean_absolute_error: 8.7452 - val_loss: 9.6916 - val_mean_absolute_error: 9.6916\n",
      "Epoch 337/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.7221 - mean_absolute_error: 8.7221\n",
      "Epoch 337: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7388 - mean_absolute_error: 8.7388 - val_loss: 9.9151 - val_mean_absolute_error: 9.9151\n",
      "Epoch 338/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.7300 - mean_absolute_error: 8.7300\n",
      "Epoch 338: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7916 - mean_absolute_error: 8.7916 - val_loss: 9.7276 - val_mean_absolute_error: 9.7276\n",
      "Epoch 339/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/136 [======================>.......] - ETA: 0s - loss: 8.7194 - mean_absolute_error: 8.7194\n",
      "Epoch 339: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7801 - mean_absolute_error: 8.7801 - val_loss: 9.6046 - val_mean_absolute_error: 9.6046\n",
      "Epoch 340/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 8.6683 - mean_absolute_error: 8.6683\n",
      "Epoch 340: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7243 - mean_absolute_error: 8.7243 - val_loss: 9.7512 - val_mean_absolute_error: 9.7512\n",
      "Epoch 341/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7542 - mean_absolute_error: 8.7542\n",
      "Epoch 341: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7474 - mean_absolute_error: 8.7474 - val_loss: 9.7237 - val_mean_absolute_error: 9.7237\n",
      "Epoch 342/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.6199 - mean_absolute_error: 8.6199\n",
      "Epoch 342: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7354 - mean_absolute_error: 8.7354 - val_loss: 9.7755 - val_mean_absolute_error: 9.7755\n",
      "Epoch 343/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.6386 - mean_absolute_error: 8.6386\n",
      "Epoch 343: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7666 - mean_absolute_error: 8.7666 - val_loss: 9.6801 - val_mean_absolute_error: 9.6801\n",
      "Epoch 344/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6889 - mean_absolute_error: 8.6889\n",
      "Epoch 344: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7272 - mean_absolute_error: 8.7272 - val_loss: 9.7953 - val_mean_absolute_error: 9.7953\n",
      "Epoch 345/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7659 - mean_absolute_error: 8.7659\n",
      "Epoch 345: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7612 - mean_absolute_error: 8.7612 - val_loss: 9.8916 - val_mean_absolute_error: 9.8916\n",
      "Epoch 346/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7834 - mean_absolute_error: 8.7834\n",
      "Epoch 346: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7466 - mean_absolute_error: 8.7466 - val_loss: 9.6890 - val_mean_absolute_error: 9.6890\n",
      "Epoch 347/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.6413 - mean_absolute_error: 8.6413\n",
      "Epoch 347: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6841 - mean_absolute_error: 8.6841 - val_loss: 9.7906 - val_mean_absolute_error: 9.7906\n",
      "Epoch 348/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.7123 - mean_absolute_error: 8.7123\n",
      "Epoch 348: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7465 - mean_absolute_error: 8.7465 - val_loss: 9.6387 - val_mean_absolute_error: 9.6387\n",
      "Epoch 349/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.8117 - mean_absolute_error: 8.8117\n",
      "Epoch 349: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7560 - mean_absolute_error: 8.7560 - val_loss: 9.7233 - val_mean_absolute_error: 9.7233\n",
      "Epoch 350/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7722 - mean_absolute_error: 8.7722\n",
      "Epoch 350: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7848 - mean_absolute_error: 8.7848 - val_loss: 9.9674 - val_mean_absolute_error: 9.9674\n",
      "Epoch 351/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7172 - mean_absolute_error: 8.7172\n",
      "Epoch 351: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7383 - mean_absolute_error: 8.7383 - val_loss: 9.6263 - val_mean_absolute_error: 9.6263\n",
      "Epoch 352/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7142 - mean_absolute_error: 8.7142\n",
      "Epoch 352: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6952 - mean_absolute_error: 8.6952 - val_loss: 9.9879 - val_mean_absolute_error: 9.9879\n",
      "Epoch 353/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.7450 - mean_absolute_error: 8.7450\n",
      "Epoch 353: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7650 - mean_absolute_error: 8.7650 - val_loss: 9.6317 - val_mean_absolute_error: 9.6317\n",
      "Epoch 354/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.7188 - mean_absolute_error: 8.7188\n",
      "Epoch 354: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7176 - mean_absolute_error: 8.7176 - val_loss: 9.6834 - val_mean_absolute_error: 9.6834\n",
      "Epoch 355/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7595 - mean_absolute_error: 8.7595\n",
      "Epoch 355: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7184 - mean_absolute_error: 8.7184 - val_loss: 9.7297 - val_mean_absolute_error: 9.7297\n",
      "Epoch 356/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.6378 - mean_absolute_error: 8.6378\n",
      "Epoch 356: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6778 - mean_absolute_error: 8.6778 - val_loss: 9.6715 - val_mean_absolute_error: 9.6715\n",
      "Epoch 357/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.7023 - mean_absolute_error: 8.7023\n",
      "Epoch 357: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6746 - mean_absolute_error: 8.6746 - val_loss: 9.6701 - val_mean_absolute_error: 9.6701\n",
      "Epoch 358/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7426 - mean_absolute_error: 8.7426\n",
      "Epoch 358: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7148 - mean_absolute_error: 8.7148 - val_loss: 9.8825 - val_mean_absolute_error: 9.8825\n",
      "Epoch 359/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6463 - mean_absolute_error: 8.6463\n",
      "Epoch 359: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7046 - mean_absolute_error: 8.7046 - val_loss: 9.7382 - val_mean_absolute_error: 9.7382\n",
      "Epoch 360/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.8376 - mean_absolute_error: 8.8376\n",
      "Epoch 360: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7847 - mean_absolute_error: 8.7847 - val_loss: 9.7383 - val_mean_absolute_error: 9.7383\n",
      "Epoch 361/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6464 - mean_absolute_error: 8.6464\n",
      "Epoch 361: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7041 - mean_absolute_error: 8.7041 - val_loss: 9.6900 - val_mean_absolute_error: 9.6900\n",
      "Epoch 362/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.5755 - mean_absolute_error: 8.5755\n",
      "Epoch 362: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6529 - mean_absolute_error: 8.6529 - val_loss: 9.6589 - val_mean_absolute_error: 9.6589\n",
      "Epoch 363/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.9102 - mean_absolute_error: 8.9102\n",
      "Epoch 363: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7667 - mean_absolute_error: 8.7667 - val_loss: 9.6694 - val_mean_absolute_error: 9.6694\n",
      "Epoch 364/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7313 - mean_absolute_error: 8.7313\n",
      "Epoch 364: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7068 - mean_absolute_error: 8.7068 - val_loss: 9.7642 - val_mean_absolute_error: 9.7642\n",
      "Epoch 365/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/136 [=======================>......] - ETA: 0s - loss: 8.6313 - mean_absolute_error: 8.6313\n",
      "Epoch 365: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6895 - mean_absolute_error: 8.6895 - val_loss: 9.8307 - val_mean_absolute_error: 9.8307\n",
      "Epoch 366/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.7199 - mean_absolute_error: 8.7199\n",
      "Epoch 366: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7127 - mean_absolute_error: 8.7127 - val_loss: 10.0105 - val_mean_absolute_error: 10.0105\n",
      "Epoch 367/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.6277 - mean_absolute_error: 8.6277\n",
      "Epoch 367: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6780 - mean_absolute_error: 8.6780 - val_loss: 9.9627 - val_mean_absolute_error: 9.9627\n",
      "Epoch 368/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6994 - mean_absolute_error: 8.6994\n",
      "Epoch 368: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7091 - mean_absolute_error: 8.7091 - val_loss: 9.7729 - val_mean_absolute_error: 9.7729\n",
      "Epoch 369/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.6620 - mean_absolute_error: 8.6620\n",
      "Epoch 369: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6697 - mean_absolute_error: 8.6697 - val_loss: 9.6473 - val_mean_absolute_error: 9.6473\n",
      "Epoch 370/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7934 - mean_absolute_error: 8.7934\n",
      "Epoch 370: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7746 - mean_absolute_error: 8.7746 - val_loss: 9.6916 - val_mean_absolute_error: 9.6916\n",
      "Epoch 371/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.6409 - mean_absolute_error: 8.6409\n",
      "Epoch 371: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7080 - mean_absolute_error: 8.7080 - val_loss: 9.8261 - val_mean_absolute_error: 9.8261\n",
      "Epoch 372/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.8123 - mean_absolute_error: 8.8123\n",
      "Epoch 372: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7404 - mean_absolute_error: 8.7404 - val_loss: 10.0484 - val_mean_absolute_error: 10.0484\n",
      "Epoch 373/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7161 - mean_absolute_error: 8.7161\n",
      "Epoch 373: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7458 - mean_absolute_error: 8.7458 - val_loss: 9.8375 - val_mean_absolute_error: 9.8375\n",
      "Epoch 374/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.7423 - mean_absolute_error: 8.7423\n",
      "Epoch 374: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6992 - mean_absolute_error: 8.6992 - val_loss: 9.6980 - val_mean_absolute_error: 9.6980\n",
      "Epoch 375/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7665 - mean_absolute_error: 8.7665\n",
      "Epoch 375: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7015 - mean_absolute_error: 8.7015 - val_loss: 9.7746 - val_mean_absolute_error: 9.7746\n",
      "Epoch 376/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.5805 - mean_absolute_error: 8.5805\n",
      "Epoch 376: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6565 - mean_absolute_error: 8.6565 - val_loss: 9.6733 - val_mean_absolute_error: 9.6733\n",
      "Epoch 377/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6319 - mean_absolute_error: 8.6319\n",
      "Epoch 377: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6869 - mean_absolute_error: 8.6869 - val_loss: 10.1736 - val_mean_absolute_error: 10.1736\n",
      "Epoch 378/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7154 - mean_absolute_error: 8.7154\n",
      "Epoch 378: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6977 - mean_absolute_error: 8.6977 - val_loss: 9.8977 - val_mean_absolute_error: 9.8977\n",
      "Epoch 379/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7483 - mean_absolute_error: 8.7483\n",
      "Epoch 379: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7636 - mean_absolute_error: 8.7636 - val_loss: 9.8657 - val_mean_absolute_error: 9.8657\n",
      "Epoch 380/500\n",
      "136/136 [==============================] - ETA: 0s - loss: 8.6255 - mean_absolute_error: 8.6255\n",
      "Epoch 380: val_loss did not improve from 9.60305\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6255 - mean_absolute_error: 8.6255 - val_loss: 9.7591 - val_mean_absolute_error: 9.7591\n",
      "Epoch 381/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.7166 - mean_absolute_error: 8.7166\n",
      "Epoch 381: val_loss improved from 9.60305 to 9.60236, saving model to Weights-381--9.60236.hdf5\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7295 - mean_absolute_error: 8.7295 - val_loss: 9.6024 - val_mean_absolute_error: 9.6024\n",
      "Epoch 382/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.7140 - mean_absolute_error: 8.7140\n",
      "Epoch 382: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7131 - mean_absolute_error: 8.7131 - val_loss: 9.6740 - val_mean_absolute_error: 9.6740\n",
      "Epoch 383/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.6293 - mean_absolute_error: 8.6293\n",
      "Epoch 383: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6801 - mean_absolute_error: 8.6801 - val_loss: 9.8177 - val_mean_absolute_error: 9.8177\n",
      "Epoch 384/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.6373 - mean_absolute_error: 8.6373\n",
      "Epoch 384: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6827 - mean_absolute_error: 8.6827 - val_loss: 9.6849 - val_mean_absolute_error: 9.6849\n",
      "Epoch 385/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6604 - mean_absolute_error: 8.6604\n",
      "Epoch 385: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7103 - mean_absolute_error: 8.7103 - val_loss: 9.6983 - val_mean_absolute_error: 9.6983\n",
      "Epoch 386/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.6776 - mean_absolute_error: 8.6776\n",
      "Epoch 386: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6721 - mean_absolute_error: 8.6721 - val_loss: 10.0027 - val_mean_absolute_error: 10.0027\n",
      "Epoch 387/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.4945 - mean_absolute_error: 8.4945\n",
      "Epoch 387: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6754 - mean_absolute_error: 8.6754 - val_loss: 9.7655 - val_mean_absolute_error: 9.7655\n",
      "Epoch 388/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6358 - mean_absolute_error: 8.6358\n",
      "Epoch 388: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6243 - mean_absolute_error: 8.6243 - val_loss: 9.8533 - val_mean_absolute_error: 9.8533\n",
      "Epoch 389/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7651 - mean_absolute_error: 8.7651\n",
      "Epoch 389: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6841 - mean_absolute_error: 8.6841 - val_loss: 9.7322 - val_mean_absolute_error: 9.7322\n",
      "Epoch 390/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6602 - mean_absolute_error: 8.6602\n",
      "Epoch 390: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6365 - mean_absolute_error: 8.6365 - val_loss: 9.7055 - val_mean_absolute_error: 9.7055\n",
      "Epoch 391/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7102 - mean_absolute_error: 8.7102\n",
      "Epoch 391: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6746 - mean_absolute_error: 8.6746 - val_loss: 9.6332 - val_mean_absolute_error: 9.6332\n",
      "Epoch 392/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6781 - mean_absolute_error: 8.6781\n",
      "Epoch 392: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6831 - mean_absolute_error: 8.6831 - val_loss: 9.6189 - val_mean_absolute_error: 9.6189\n",
      "Epoch 393/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6263 - mean_absolute_error: 8.6263\n",
      "Epoch 393: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6306 - mean_absolute_error: 8.6306 - val_loss: 9.6881 - val_mean_absolute_error: 9.6881\n",
      "Epoch 394/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.6001 - mean_absolute_error: 8.6001\n",
      "Epoch 394: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6711 - mean_absolute_error: 8.6711 - val_loss: 9.6912 - val_mean_absolute_error: 9.6912\n",
      "Epoch 395/500\n",
      "121/136 [=========================>....] - ETA: 0s - loss: 8.6596 - mean_absolute_error: 8.6596\n",
      "Epoch 395: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6817 - mean_absolute_error: 8.6817 - val_loss: 9.7021 - val_mean_absolute_error: 9.7021\n",
      "Epoch 396/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.6923 - mean_absolute_error: 8.6923\n",
      "Epoch 396: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7029 - mean_absolute_error: 8.7029 - val_loss: 9.8382 - val_mean_absolute_error: 9.8382\n",
      "Epoch 397/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6509 - mean_absolute_error: 8.6509\n",
      "Epoch 397: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6588 - mean_absolute_error: 8.6588 - val_loss: 9.7813 - val_mean_absolute_error: 9.7813\n",
      "Epoch 398/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6751 - mean_absolute_error: 8.6751\n",
      "Epoch 398: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6765 - mean_absolute_error: 8.6765 - val_loss: 9.7628 - val_mean_absolute_error: 9.7628\n",
      "Epoch 399/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.5038 - mean_absolute_error: 8.5038\n",
      "Epoch 399: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6050 - mean_absolute_error: 8.6050 - val_loss: 9.8394 - val_mean_absolute_error: 9.8394\n",
      "Epoch 400/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6763 - mean_absolute_error: 8.6763\n",
      "Epoch 400: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7014 - mean_absolute_error: 8.7014 - val_loss: 9.6274 - val_mean_absolute_error: 9.6274\n",
      "Epoch 401/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.7239 - mean_absolute_error: 8.7239\n",
      "Epoch 401: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6934 - mean_absolute_error: 8.6934 - val_loss: 9.6761 - val_mean_absolute_error: 9.6761\n",
      "Epoch 402/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.7339 - mean_absolute_error: 8.7339\n",
      "Epoch 402: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6873 - mean_absolute_error: 8.6873 - val_loss: 9.8915 - val_mean_absolute_error: 9.8915\n",
      "Epoch 403/500\n",
      "135/136 [============================>.] - ETA: 0s - loss: 8.6984 - mean_absolute_error: 8.6984\n",
      "Epoch 403: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6990 - mean_absolute_error: 8.6990 - val_loss: 9.9031 - val_mean_absolute_error: 9.9031\n",
      "Epoch 404/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.6029 - mean_absolute_error: 8.6029\n",
      "Epoch 404: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6627 - mean_absolute_error: 8.6627 - val_loss: 9.6957 - val_mean_absolute_error: 9.6957\n",
      "Epoch 405/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.6127 - mean_absolute_error: 8.6127\n",
      "Epoch 405: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6503 - mean_absolute_error: 8.6503 - val_loss: 9.7137 - val_mean_absolute_error: 9.7137\n",
      "Epoch 406/500\n",
      "118/136 [=========================>....] - ETA: 0s - loss: 8.7242 - mean_absolute_error: 8.7242\n",
      "Epoch 406: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6531 - mean_absolute_error: 8.6531 - val_loss: 9.6259 - val_mean_absolute_error: 9.6259\n",
      "Epoch 407/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6790 - mean_absolute_error: 8.6790\n",
      "Epoch 407: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6693 - mean_absolute_error: 8.6693 - val_loss: 9.7150 - val_mean_absolute_error: 9.7150\n",
      "Epoch 408/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.7943 - mean_absolute_error: 8.7943\n",
      "Epoch 408: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6568 - mean_absolute_error: 8.6568 - val_loss: 9.7620 - val_mean_absolute_error: 9.7620\n",
      "Epoch 409/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.6536 - mean_absolute_error: 8.6536\n",
      "Epoch 409: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6718 - mean_absolute_error: 8.6718 - val_loss: 9.7218 - val_mean_absolute_error: 9.7218\n",
      "Epoch 410/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.6425 - mean_absolute_error: 8.6425\n",
      "Epoch 410: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6189 - mean_absolute_error: 8.6189 - val_loss: 9.6056 - val_mean_absolute_error: 9.6056\n",
      "Epoch 411/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.7484 - mean_absolute_error: 8.7484\n",
      "Epoch 411: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6712 - mean_absolute_error: 8.6712 - val_loss: 10.1069 - val_mean_absolute_error: 10.1069\n",
      "Epoch 412/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6646 - mean_absolute_error: 8.6646\n",
      "Epoch 412: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7021 - mean_absolute_error: 8.7021 - val_loss: 9.8114 - val_mean_absolute_error: 9.8114\n",
      "Epoch 413/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6715 - mean_absolute_error: 8.6715\n",
      "Epoch 413: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6613 - mean_absolute_error: 8.6613 - val_loss: 9.8623 - val_mean_absolute_error: 9.8623\n",
      "Epoch 414/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.7076 - mean_absolute_error: 8.7076\n",
      "Epoch 414: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6569 - mean_absolute_error: 8.6569 - val_loss: 9.9779 - val_mean_absolute_error: 9.9779\n",
      "Epoch 415/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.8219 - mean_absolute_error: 8.8219\n",
      "Epoch 415: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7521 - mean_absolute_error: 8.7521 - val_loss: 9.6726 - val_mean_absolute_error: 9.6726\n",
      "Epoch 416/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6731 - mean_absolute_error: 8.6731\n",
      "Epoch 416: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6781 - mean_absolute_error: 8.6781 - val_loss: 9.8194 - val_mean_absolute_error: 9.8194\n",
      "Epoch 417/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/136 [=======================>......] - ETA: 0s - loss: 8.6922 - mean_absolute_error: 8.6922\n",
      "Epoch 417: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6553 - mean_absolute_error: 8.6553 - val_loss: 9.7530 - val_mean_absolute_error: 9.7530\n",
      "Epoch 418/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7071 - mean_absolute_error: 8.7071\n",
      "Epoch 418: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6677 - mean_absolute_error: 8.6677 - val_loss: 9.7259 - val_mean_absolute_error: 9.7259\n",
      "Epoch 419/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5165 - mean_absolute_error: 8.5165\n",
      "Epoch 419: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5797 - mean_absolute_error: 8.5797 - val_loss: 9.6502 - val_mean_absolute_error: 9.6502\n",
      "Epoch 420/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6445 - mean_absolute_error: 8.6445\n",
      "Epoch 420: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6624 - mean_absolute_error: 8.6624 - val_loss: 9.8202 - val_mean_absolute_error: 9.8202\n",
      "Epoch 421/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5623 - mean_absolute_error: 8.5623\n",
      "Epoch 421: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5859 - mean_absolute_error: 8.5859 - val_loss: 9.6709 - val_mean_absolute_error: 9.6709\n",
      "Epoch 422/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.7693 - mean_absolute_error: 8.7693\n",
      "Epoch 422: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6254 - mean_absolute_error: 8.6254 - val_loss: 9.7454 - val_mean_absolute_error: 9.7454\n",
      "Epoch 423/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.7461 - mean_absolute_error: 8.7461\n",
      "Epoch 423: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6763 - mean_absolute_error: 8.6763 - val_loss: 9.7159 - val_mean_absolute_error: 9.7159\n",
      "Epoch 424/500\n",
      "125/136 [==========================>...] - ETA: 0s - loss: 8.6398 - mean_absolute_error: 8.6398\n",
      "Epoch 424: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6117 - mean_absolute_error: 8.6117 - val_loss: 9.7515 - val_mean_absolute_error: 9.7515\n",
      "Epoch 425/500\n",
      "105/136 [======================>.......] - ETA: 0s - loss: 8.6182 - mean_absolute_error: 8.6182\n",
      "Epoch 425: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5919 - mean_absolute_error: 8.5919 - val_loss: 9.6606 - val_mean_absolute_error: 9.6606\n",
      "Epoch 426/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.7227 - mean_absolute_error: 8.7227\n",
      "Epoch 426: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6069 - mean_absolute_error: 8.6069 - val_loss: 9.7376 - val_mean_absolute_error: 9.7376\n",
      "Epoch 427/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5832 - mean_absolute_error: 8.5832\n",
      "Epoch 427: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6323 - mean_absolute_error: 8.6323 - val_loss: 9.7032 - val_mean_absolute_error: 9.7032\n",
      "Epoch 428/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7548 - mean_absolute_error: 8.7548\n",
      "Epoch 428: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6744 - mean_absolute_error: 8.6744 - val_loss: 9.7338 - val_mean_absolute_error: 9.7338\n",
      "Epoch 429/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6223 - mean_absolute_error: 8.6223\n",
      "Epoch 429: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6294 - mean_absolute_error: 8.6294 - val_loss: 9.6647 - val_mean_absolute_error: 9.6647\n",
      "Epoch 430/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7339 - mean_absolute_error: 8.7339\n",
      "Epoch 430: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6608 - mean_absolute_error: 8.6608 - val_loss: 9.7561 - val_mean_absolute_error: 9.7561\n",
      "Epoch 431/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6710 - mean_absolute_error: 8.6710\n",
      "Epoch 431: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6192 - mean_absolute_error: 8.6192 - val_loss: 9.7533 - val_mean_absolute_error: 9.7533\n",
      "Epoch 432/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6306 - mean_absolute_error: 8.6306\n",
      "Epoch 432: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6076 - mean_absolute_error: 8.6076 - val_loss: 9.8559 - val_mean_absolute_error: 9.8559\n",
      "Epoch 433/500\n",
      "103/136 [=====================>........] - ETA: 0s - loss: 8.6799 - mean_absolute_error: 8.6799\n",
      "Epoch 433: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6291 - mean_absolute_error: 8.6291 - val_loss: 9.8688 - val_mean_absolute_error: 9.8688\n",
      "Epoch 434/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.5667 - mean_absolute_error: 8.5667\n",
      "Epoch 434: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6081 - mean_absolute_error: 8.6081 - val_loss: 9.8521 - val_mean_absolute_error: 9.8521\n",
      "Epoch 435/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6300 - mean_absolute_error: 8.6300\n",
      "Epoch 435: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6363 - mean_absolute_error: 8.6363 - val_loss: 9.7642 - val_mean_absolute_error: 9.7642\n",
      "Epoch 436/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.7386 - mean_absolute_error: 8.7386\n",
      "Epoch 436: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.7569 - mean_absolute_error: 8.7569 - val_loss: 9.7270 - val_mean_absolute_error: 9.7270\n",
      "Epoch 437/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6694 - mean_absolute_error: 8.6694\n",
      "Epoch 437: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6338 - mean_absolute_error: 8.6338 - val_loss: 9.6871 - val_mean_absolute_error: 9.6871\n",
      "Epoch 438/500\n",
      "117/136 [========================>.....] - ETA: 0s - loss: 8.6668 - mean_absolute_error: 8.6668\n",
      "Epoch 438: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5845 - mean_absolute_error: 8.5845 - val_loss: 10.2404 - val_mean_absolute_error: 10.2404\n",
      "Epoch 439/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5750 - mean_absolute_error: 8.5750\n",
      "Epoch 439: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6491 - mean_absolute_error: 8.6491 - val_loss: 9.7591 - val_mean_absolute_error: 9.7591\n",
      "Epoch 440/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5969 - mean_absolute_error: 8.5969\n",
      "Epoch 440: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6123 - mean_absolute_error: 8.6123 - val_loss: 9.8023 - val_mean_absolute_error: 9.8023\n",
      "Epoch 441/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.6246 - mean_absolute_error: 8.6246\n",
      "Epoch 441: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6252 - mean_absolute_error: 8.6252 - val_loss: 9.7140 - val_mean_absolute_error: 9.7140\n",
      "Epoch 442/500\n",
      "107/136 [======================>.......] - ETA: 0s - loss: 8.6744 - mean_absolute_error: 8.6744\n",
      "Epoch 442: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6430 - mean_absolute_error: 8.6430 - val_loss: 9.6720 - val_mean_absolute_error: 9.6720\n",
      "Epoch 443/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/136 [======================>.......] - ETA: 0s - loss: 8.5903 - mean_absolute_error: 8.5903\n",
      "Epoch 443: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6401 - mean_absolute_error: 8.6401 - val_loss: 9.7124 - val_mean_absolute_error: 9.7124\n",
      "Epoch 444/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6165 - mean_absolute_error: 8.6165\n",
      "Epoch 444: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6142 - mean_absolute_error: 8.6142 - val_loss: 9.9015 - val_mean_absolute_error: 9.9015\n",
      "Epoch 445/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.5679 - mean_absolute_error: 8.5679\n",
      "Epoch 445: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5981 - mean_absolute_error: 8.5981 - val_loss: 9.8912 - val_mean_absolute_error: 9.8912\n",
      "Epoch 446/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5879 - mean_absolute_error: 8.5879\n",
      "Epoch 446: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5886 - mean_absolute_error: 8.5886 - val_loss: 9.8379 - val_mean_absolute_error: 9.8379\n",
      "Epoch 447/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.6337 - mean_absolute_error: 8.6337\n",
      "Epoch 447: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6302 - mean_absolute_error: 8.6302 - val_loss: 9.9400 - val_mean_absolute_error: 9.9400\n",
      "Epoch 448/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.7031 - mean_absolute_error: 8.7031\n",
      "Epoch 448: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6795 - mean_absolute_error: 8.6795 - val_loss: 9.7856 - val_mean_absolute_error: 9.7856\n",
      "Epoch 449/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.7225 - mean_absolute_error: 8.7225\n",
      "Epoch 449: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6640 - mean_absolute_error: 8.6640 - val_loss: 9.7575 - val_mean_absolute_error: 9.7575\n",
      "Epoch 450/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.6189 - mean_absolute_error: 8.6189\n",
      "Epoch 450: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5906 - mean_absolute_error: 8.5906 - val_loss: 9.8830 - val_mean_absolute_error: 9.8830\n",
      "Epoch 451/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.7002 - mean_absolute_error: 8.7002\n",
      "Epoch 451: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6368 - mean_absolute_error: 8.6368 - val_loss: 9.7065 - val_mean_absolute_error: 9.7065\n",
      "Epoch 452/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5675 - mean_absolute_error: 8.5675\n",
      "Epoch 452: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5980 - mean_absolute_error: 8.5980 - val_loss: 9.8596 - val_mean_absolute_error: 9.8596\n",
      "Epoch 453/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5882 - mean_absolute_error: 8.5882\n",
      "Epoch 453: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6586 - mean_absolute_error: 8.6586 - val_loss: 9.7529 - val_mean_absolute_error: 9.7529\n",
      "Epoch 454/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.5459 - mean_absolute_error: 8.5459\n",
      "Epoch 454: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5937 - mean_absolute_error: 8.5937 - val_loss: 9.7769 - val_mean_absolute_error: 9.7769\n",
      "Epoch 455/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5874 - mean_absolute_error: 8.5874\n",
      "Epoch 455: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5996 - mean_absolute_error: 8.5996 - val_loss: 9.9939 - val_mean_absolute_error: 9.9939\n",
      "Epoch 456/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6551 - mean_absolute_error: 8.6551\n",
      "Epoch 456: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6326 - mean_absolute_error: 8.6326 - val_loss: 9.6442 - val_mean_absolute_error: 9.6442\n",
      "Epoch 457/500\n",
      "120/136 [=========================>....] - ETA: 0s - loss: 8.6877 - mean_absolute_error: 8.6877\n",
      "Epoch 457: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6190 - mean_absolute_error: 8.6190 - val_loss: 9.8387 - val_mean_absolute_error: 9.8387\n",
      "Epoch 458/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.5178 - mean_absolute_error: 8.5178\n",
      "Epoch 458: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6034 - mean_absolute_error: 8.6034 - val_loss: 9.8253 - val_mean_absolute_error: 9.8253\n",
      "Epoch 459/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6504 - mean_absolute_error: 8.6504\n",
      "Epoch 459: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6129 - mean_absolute_error: 8.6129 - val_loss: 9.7139 - val_mean_absolute_error: 9.7139\n",
      "Epoch 460/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5664 - mean_absolute_error: 8.5664\n",
      "Epoch 460: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5726 - mean_absolute_error: 8.5726 - val_loss: 9.6788 - val_mean_absolute_error: 9.6788\n",
      "Epoch 461/500\n",
      "136/136 [==============================] - ETA: 0s - loss: 8.6197 - mean_absolute_error: 8.6197\n",
      "Epoch 461: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6197 - mean_absolute_error: 8.6197 - val_loss: 9.7667 - val_mean_absolute_error: 9.7667\n",
      "Epoch 462/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5578 - mean_absolute_error: 8.5578\n",
      "Epoch 462: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5927 - mean_absolute_error: 8.5927 - val_loss: 9.7510 - val_mean_absolute_error: 9.7510\n",
      "Epoch 463/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.5403 - mean_absolute_error: 8.5403\n",
      "Epoch 463: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5882 - mean_absolute_error: 8.5882 - val_loss: 9.9802 - val_mean_absolute_error: 9.9802\n",
      "Epoch 464/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.5918 - mean_absolute_error: 8.5918\n",
      "Epoch 464: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6078 - mean_absolute_error: 8.6078 - val_loss: 9.6581 - val_mean_absolute_error: 9.6581\n",
      "Epoch 465/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6443 - mean_absolute_error: 8.6443\n",
      "Epoch 465: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5986 - mean_absolute_error: 8.5986 - val_loss: 9.7394 - val_mean_absolute_error: 9.7394\n",
      "Epoch 466/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6444 - mean_absolute_error: 8.6444\n",
      "Epoch 466: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5969 - mean_absolute_error: 8.5969 - val_loss: 9.6810 - val_mean_absolute_error: 9.6810\n",
      "Epoch 467/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.5948 - mean_absolute_error: 8.5948\n",
      "Epoch 467: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5683 - mean_absolute_error: 8.5683 - val_loss: 9.9700 - val_mean_absolute_error: 9.9700\n",
      "Epoch 468/500\n",
      "109/136 [=======================>......] - ETA: 0s - loss: 8.6098 - mean_absolute_error: 8.6098\n",
      "Epoch 468: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6050 - mean_absolute_error: 8.6050 - val_loss: 9.6930 - val_mean_absolute_error: 9.6930\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/136 [=======================>......] - ETA: 0s - loss: 8.4678 - mean_absolute_error: 8.4678\n",
      "Epoch 469: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5869 - mean_absolute_error: 8.5869 - val_loss: 10.0442 - val_mean_absolute_error: 10.0442\n",
      "Epoch 470/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.5686 - mean_absolute_error: 8.5686\n",
      "Epoch 470: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6102 - mean_absolute_error: 8.6102 - val_loss: 9.7241 - val_mean_absolute_error: 9.7241\n",
      "Epoch 471/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.6824 - mean_absolute_error: 8.6824\n",
      "Epoch 471: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6771 - mean_absolute_error: 8.6771 - val_loss: 9.7159 - val_mean_absolute_error: 9.7159\n",
      "Epoch 472/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.6195 - mean_absolute_error: 8.6195\n",
      "Epoch 472: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6086 - mean_absolute_error: 8.6086 - val_loss: 9.6528 - val_mean_absolute_error: 9.6528\n",
      "Epoch 473/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5202 - mean_absolute_error: 8.5202\n",
      "Epoch 473: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5721 - mean_absolute_error: 8.5721 - val_loss: 9.7335 - val_mean_absolute_error: 9.7335\n",
      "Epoch 474/500\n",
      "113/136 [=======================>......] - ETA: 0s - loss: 8.5670 - mean_absolute_error: 8.5670\n",
      "Epoch 474: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5505 - mean_absolute_error: 8.5505 - val_loss: 9.7236 - val_mean_absolute_error: 9.7236\n",
      "Epoch 475/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.4607 - mean_absolute_error: 8.4607\n",
      "Epoch 475: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5376 - mean_absolute_error: 8.5376 - val_loss: 9.7468 - val_mean_absolute_error: 9.7468\n",
      "Epoch 476/500\n",
      "108/136 [======================>.......] - ETA: 0s - loss: 8.4012 - mean_absolute_error: 8.4012\n",
      "Epoch 476: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5484 - mean_absolute_error: 8.5484 - val_loss: 9.7458 - val_mean_absolute_error: 9.7458\n",
      "Epoch 477/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.7426 - mean_absolute_error: 8.7426\n",
      "Epoch 477: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6551 - mean_absolute_error: 8.6551 - val_loss: 9.6581 - val_mean_absolute_error: 9.6581\n",
      "Epoch 478/500\n",
      "101/136 [=====================>........] - ETA: 0s - loss: 8.4810 - mean_absolute_error: 8.4810\n",
      "Epoch 478: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6178 - mean_absolute_error: 8.6178 - val_loss: 9.9765 - val_mean_absolute_error: 9.9765\n",
      "Epoch 479/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5287 - mean_absolute_error: 8.5287\n",
      "Epoch 479: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5920 - mean_absolute_error: 8.5920 - val_loss: 9.7968 - val_mean_absolute_error: 9.7968\n",
      "Epoch 480/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.4535 - mean_absolute_error: 8.4535\n",
      "Epoch 480: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5441 - mean_absolute_error: 8.5441 - val_loss: 9.6263 - val_mean_absolute_error: 9.6263\n",
      "Epoch 481/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.5139 - mean_absolute_error: 8.5139\n",
      "Epoch 481: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5454 - mean_absolute_error: 8.5454 - val_loss: 10.1300 - val_mean_absolute_error: 10.1300\n",
      "Epoch 482/500\n",
      "119/136 [=========================>....] - ETA: 0s - loss: 8.6010 - mean_absolute_error: 8.6010\n",
      "Epoch 482: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6030 - mean_absolute_error: 8.6030 - val_loss: 10.0400 - val_mean_absolute_error: 10.0400\n",
      "Epoch 483/500\n",
      "111/136 [=======================>......] - ETA: 0s - loss: 8.6294 - mean_absolute_error: 8.6294\n",
      "Epoch 483: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5484 - mean_absolute_error: 8.5484 - val_loss: 9.8079 - val_mean_absolute_error: 9.8079\n",
      "Epoch 484/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5925 - mean_absolute_error: 8.5925\n",
      "Epoch 484: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6310 - mean_absolute_error: 8.6310 - val_loss: 9.8155 - val_mean_absolute_error: 9.8155\n",
      "Epoch 485/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.5955 - mean_absolute_error: 8.5955\n",
      "Epoch 485: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5665 - mean_absolute_error: 8.5665 - val_loss: 9.7346 - val_mean_absolute_error: 9.7346\n",
      "Epoch 486/500\n",
      "110/136 [=======================>......] - ETA: 0s - loss: 8.5163 - mean_absolute_error: 8.5163\n",
      "Epoch 486: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5549 - mean_absolute_error: 8.5549 - val_loss: 9.9408 - val_mean_absolute_error: 9.9408\n",
      "Epoch 487/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5408 - mean_absolute_error: 8.5408\n",
      "Epoch 487: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5861 - mean_absolute_error: 8.5861 - val_loss: 9.8585 - val_mean_absolute_error: 9.8585\n",
      "Epoch 488/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.5410 - mean_absolute_error: 8.5410\n",
      "Epoch 488: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5922 - mean_absolute_error: 8.5922 - val_loss: 9.8201 - val_mean_absolute_error: 9.8201\n",
      "Epoch 489/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5618 - mean_absolute_error: 8.5618\n",
      "Epoch 489: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5702 - mean_absolute_error: 8.5702 - val_loss: 9.6815 - val_mean_absolute_error: 9.6815\n",
      "Epoch 490/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.5728 - mean_absolute_error: 8.5728\n",
      "Epoch 490: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5894 - mean_absolute_error: 8.5894 - val_loss: 9.7875 - val_mean_absolute_error: 9.7875\n",
      "Epoch 491/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5771 - mean_absolute_error: 8.5771\n",
      "Epoch 491: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5287 - mean_absolute_error: 8.5287 - val_loss: 9.8276 - val_mean_absolute_error: 9.8276\n",
      "Epoch 492/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.7157 - mean_absolute_error: 8.7157\n",
      "Epoch 492: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6187 - mean_absolute_error: 8.6187 - val_loss: 9.7676 - val_mean_absolute_error: 9.7676\n",
      "Epoch 493/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.4858 - mean_absolute_error: 8.4858\n",
      "Epoch 493: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5465 - mean_absolute_error: 8.5465 - val_loss: 9.8025 - val_mean_absolute_error: 9.8025\n",
      "Epoch 494/500\n",
      "112/136 [=======================>......] - ETA: 0s - loss: 8.5521 - mean_absolute_error: 8.5521\n",
      "Epoch 494: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5589 - mean_absolute_error: 8.5589 - val_loss: 9.8748 - val_mean_absolute_error: 9.8748\n",
      "Epoch 495/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/136 [========================>.....] - ETA: 0s - loss: 8.5894 - mean_absolute_error: 8.5894\n",
      "Epoch 495: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5957 - mean_absolute_error: 8.5957 - val_loss: 9.7171 - val_mean_absolute_error: 9.7171\n",
      "Epoch 496/500\n",
      "104/136 [=====================>........] - ETA: 0s - loss: 8.5571 - mean_absolute_error: 8.5571\n",
      "Epoch 496: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5990 - mean_absolute_error: 8.5990 - val_loss: 10.0451 - val_mean_absolute_error: 10.0451\n",
      "Epoch 497/500\n",
      "114/136 [========================>.....] - ETA: 0s - loss: 8.6446 - mean_absolute_error: 8.6446\n",
      "Epoch 497: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6261 - mean_absolute_error: 8.6261 - val_loss: 9.6751 - val_mean_absolute_error: 9.6751\n",
      "Epoch 498/500\n",
      "116/136 [========================>.....] - ETA: 0s - loss: 8.5511 - mean_absolute_error: 8.5511\n",
      "Epoch 498: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5241 - mean_absolute_error: 8.5241 - val_loss: 9.9471 - val_mean_absolute_error: 9.9471\n",
      "Epoch 499/500\n",
      "106/136 [======================>.......] - ETA: 0s - loss: 8.5566 - mean_absolute_error: 8.5566\n",
      "Epoch 499: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.5995 - mean_absolute_error: 8.5995 - val_loss: 10.0186 - val_mean_absolute_error: 10.0186\n",
      "Epoch 500/500\n",
      "115/136 [========================>.....] - ETA: 0s - loss: 8.6881 - mean_absolute_error: 8.6881\n",
      "Epoch 500: val_loss did not improve from 9.60236\n",
      "136/136 [==============================] - 0s 2ms/step - loss: 8.6146 - mean_absolute_error: 8.6146 - val_loss: 9.7981 - val_mean_absolute_error: 9.7981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1498850bb20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(X, y, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 810us/step\n"
     ]
    }
   ],
   "source": [
    "weights_file = 'Weights-381--9.60236.hdf5'\n",
    "NN_model.load_weights(weights_file)\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "predsNN = NN_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficients optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predXGB = np.average(np.array(predsXGB),axis=0)\n",
    "predCB = np.average(np.array(predsCB),axis=0)\n",
    "predLasso = np.average(np.array(predsLasso),axis=0)\n",
    "predLGBM = np.average(np.array(predsLGBM),axis=0)\n",
    "predSVR = np.average(np.array(predsSVR),axis=0)\n",
    "predRF = np.average(np.array(predsRF),axis=0)\n",
    "predNN = np.average(np.array(predsNN),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:33:21,845]\u001b[0m A new study created in memory with name: no-name-86af937c-e841-4520-8109-fac4e253f107\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 764us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:33:31,368]\u001b[0m Trial 0 finished with value: 12.367012468596661 and parameters: {'a': 0.6406995315438081, 'b': 0.6726178414884388, 'c': 0.4088061465767693, 'd': 0.6758338136286354, 'e': 0.6397287686754, 'f': 0.9122021938952177, 'g': 0.9833103447612771}. Best is trial 0 with value: 12.367012468596661.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 695us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:33:40,909]\u001b[0m Trial 1 finished with value: 12.4624404368488 and parameters: {'a': 0.5898947634919887, 'b': 0.1226372231099011, 'c': 0.586098317460419, 'd': 0.37845929239707843, 'e': 0.11585889286178985, 'f': 0.755325366171876, 'g': 0.7221331930426036}. Best is trial 0 with value: 12.367012468596661.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 676us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:33:50,610]\u001b[0m Trial 2 finished with value: 12.140063390017524 and parameters: {'a': 0.14631825170727164, 'b': 0.520815216078479, 'c': 0.07846942709036553, 'd': 0.5831264661477094, 'e': 0.5898664559520619, 'f': 0.700074563332157, 'g': 0.3758873075986099}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 874us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:00,018]\u001b[0m Trial 3 finished with value: 12.315533277700181 and parameters: {'a': 0.9132863434791483, 'b': 0.6644061511875188, 'c': 0.700311155031225, 'd': 0.3969740438562698, 'e': 0.2672247690102837, 'f': 0.30884760171425496, 'g': 0.5848052143501897}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 685us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:09,946]\u001b[0m Trial 4 finished with value: 12.545246498732832 and parameters: {'a': 0.16673772622500527, 'b': 0.19740246829736607, 'c': 0.6114295028462048, 'd': 0.8963337057153365, 'e': 0.7582404402829251, 'f': 0.9042797964543474, 'g': 0.7893168618584578}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 702us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:19,481]\u001b[0m Trial 5 finished with value: 12.449943598045097 and parameters: {'a': 0.7263842083224193, 'b': 0.2048153970696992, 'c': 0.9917365231643249, 'd': 0.3622336769214085, 'e': 0.06607900213824025, 'f': 0.3787490923982759, 'g': 0.3737799709255525}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 694us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:29,141]\u001b[0m Trial 6 finished with value: 12.57495173434117 and parameters: {'a': 0.2578484922930917, 'b': 0.1844042358510769, 'c': 0.13881028622799518, 'd': 0.6855011322801012, 'e': 0.3316472340128842, 'f': 0.14197166363641966, 'g': 0.6193799551281277}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 781us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:38,641]\u001b[0m Trial 7 finished with value: 12.250966948567141 and parameters: {'a': 0.893556927837009, 'b': 0.9722340919310691, 'c': 0.7641566546581788, 'd': 0.6113896238824398, 'e': 0.16130088934199227, 'f': 0.8597254327818691, 'g': 0.9220416140485965}. Best is trial 2 with value: 12.140063390017524.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 671us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:48,116]\u001b[0m Trial 8 finished with value: 12.039961702596775 and parameters: {'a': 0.3274764923043524, 'b': 0.18779397948342158, 'c': 0.159443662752908, 'd': 0.9603762788805601, 'e': 0.6071533076766199, 'f': 0.6062303974848797, 'g': 0.26817635527407024}. Best is trial 8 with value: 12.039961702596775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 667us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:34:57,566]\u001b[0m Trial 9 finished with value: 12.583214562453959 and parameters: {'a': 0.7415860491664089, 'b': 0.7049900160296722, 'c': 0.5903718790122763, 'd': 0.2207567972149561, 'e': 0.9368994845348566, 'f': 0.41514047247232644, 'g': 0.725495102874278}. Best is trial 8 with value: 12.039961702596775.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 733us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:07,015]\u001b[0m Trial 10 finished with value: 11.912050240695919 and parameters: {'a': 0.344835388435938, 'b': 0.001114501006794888, 'c': 0.28393396249932634, 'd': 0.9766742868617097, 'e': 0.4442997445998107, 'f': 0.574825291749156, 'g': 0.05980397261427811}. Best is trial 10 with value: 11.912050240695919.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 742us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:16,914]\u001b[0m Trial 11 finished with value: 11.912967266294814 and parameters: {'a': 0.4004836274372189, 'b': 0.004651468131334625, 'c': 0.31587317065284953, 'd': 0.9672667516344052, 'e': 0.4491611950566564, 'f': 0.6048249258819002, 'g': 0.05839459944489277}. Best is trial 10 with value: 11.912050240695919.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 677us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:26,428]\u001b[0m Trial 12 finished with value: 11.874261116121794 and parameters: {'a': 0.4266085169602024, 'b': 0.0015155956574728346, 'c': 0.337122554360206, 'd': 0.9930111347677456, 'e': 0.40120775195381986, 'f': 0.5578837965265305, 'g': 0.012143875726986136}. Best is trial 12 with value: 11.874261116121794.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 742us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:36,200]\u001b[0m Trial 13 finished with value: 11.853567824828387 and parameters: {'a': 0.45343806642957823, 'b': 0.3559478379195703, 'c': 0.28286090890263194, 'd': 0.8324568146492949, 'e': 0.41834855690963424, 'f': 0.5170199027924484, 'g': 0.015979355392141403}. Best is trial 13 with value: 11.853567824828387.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 658us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:45,719]\u001b[0m Trial 14 finished with value: 11.76382503882756 and parameters: {'a': 0.0037424264747009106, 'b': 0.3667507914647572, 'c': 0.010951599386785138, 'd': 0.826583771580078, 'e': 0.32698457178083085, 'f': 0.4928945188724933, 'g': 0.006096760012783592}. Best is trial 14 with value: 11.76382503882756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 854us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:35:55,374]\u001b[0m Trial 15 finished with value: 11.874152523049828 and parameters: {'a': 0.05628131994691924, 'b': 0.36581826972927767, 'c': 0.025705302020943566, 'd': 0.8023515828095956, 'e': 0.2562442664098014, 'f': 0.446348143530511, 'g': 0.16736342044412145}. Best is trial 14 with value: 11.76382503882756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 823us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:04,930]\u001b[0m Trial 16 finished with value: 11.944649620310422 and parameters: {'a': 0.034262567353711866, 'b': 0.3471010115252115, 'c': 0.20284940351736652, 'd': 0.8073229210623529, 'e': 0.004383893277072115, 'f': 0.2517913886322257, 'g': 0.19486052914507812}. Best is trial 14 with value: 11.76382503882756.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 649us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:14,600]\u001b[0m Trial 17 finished with value: 11.760500900413795 and parameters: {'a': 0.47624449877561054, 'b': 0.35050449817396095, 'c': 0.037876932052198664, 'd': 0.794449501234042, 'e': 0.34070552172976687, 'f': 0.004424901602261833, 'g': 0.000957753030633229}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 687us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:24,133]\u001b[0m Trial 18 finished with value: 11.799213322714323 and parameters: {'a': 0.5397885745923325, 'b': 0.4341769361092034, 'c': 0.0021791697907784305, 'd': 0.738734212353465, 'e': 0.18914411137294013, 'f': 0.004030077206443141, 'g': 0.1508618527160553}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 757us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:33,935]\u001b[0m Trial 19 finished with value: 12.224864289004243 and parameters: {'a': 0.21438813277001112, 'b': 0.5329796928976629, 'c': 0.09883099627614554, 'd': 0.5891456226217612, 'e': 0.3228375955321505, 'f': 0.02740305956272734, 'g': 0.3461229892531353}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 707us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:43,520]\u001b[0m Trial 20 finished with value: 12.373025709221487 and parameters: {'a': 0.09448959599913853, 'b': 0.2703996089246011, 'c': 0.0074676119747748345, 'd': 0.8402187242683562, 'e': 0.5105948740907952, 'f': 0.17189372540464903, 'g': 0.46671970138448804}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 644us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:36:53,140]\u001b[0m Trial 21 finished with value: 11.79317989869121 and parameters: {'a': 0.5211506598465856, 'b': 0.4307193485433012, 'c': 0.004372172550836933, 'd': 0.7293998193781244, 'e': 0.2079637102668611, 'f': 0.0009436854125422639, 'g': 0.12801997231948137}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 657us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:02,733]\u001b[0m Trial 22 finished with value: 11.907324367580564 and parameters: {'a': 0.2933205722691699, 'b': 0.4535409397378226, 'c': 0.19547366514713982, 'd': 0.735454028060201, 'e': 0.2115089263977686, 'f': 0.10550936829554257, 'g': 0.10925671751241472}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 651us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:12,101]\u001b[0m Trial 23 finished with value: 11.919581901637548 and parameters: {'a': 0.4966854601406034, 'b': 0.28885840761020903, 'c': 0.09329833023514898, 'd': 0.871012749144679, 'e': 0.32642966540941676, 'f': 0.23584102473636515, 'g': 0.2249226991908108}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 667us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:21,656]\u001b[0m Trial 24 finished with value: 11.783241029119624 and parameters: {'a': 0.22919901559855135, 'b': 0.45667419220599353, 'c': 0.00012367545063385853, 'd': 0.7519898114080814, 'e': 0.1150044864273047, 'f': 0.07147634322534989, 'g': 0.11892623633730484}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 701us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:31,174]\u001b[0m Trial 25 finished with value: 11.935539462928507 and parameters: {'a': 0.0255526925717468, 'b': 0.5411183779160027, 'c': 0.20757311276104184, 'd': 0.5280588577217571, 'e': 0.05021679096509636, 'f': 0.11789383871047593, 'g': 0.08780788546047659}. Best is trial 17 with value: 11.760500900413795.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 705us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:40,805]\u001b[0m Trial 26 finished with value: 11.708325200100223 and parameters: {'a': 0.22400549564175465, 'b': 0.28960516104037176, 'c': 0.08003747997789903, 'd': 0.9045536197143809, 'e': 0.10829292958358527, 'f': 0.30976448908835535, 'g': 0.011243140466484015}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 681us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:37:50,503]\u001b[0m Trial 27 finished with value: 12.016821207447226 and parameters: {'a': 0.12452655479453989, 'b': 0.27836716929748406, 'c': 0.11190225778570805, 'd': 0.849704170365402, 'e': 0.2900693682189639, 'f': 0.3475166253579671, 'g': 0.25293413388267433}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 690us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:00,154]\u001b[0m Trial 28 finished with value: 11.870985081855233 and parameters: {'a': 0.1921905197447648, 'b': 0.11553044725528333, 'c': 0.2381100789887354, 'd': 0.903535996499753, 'e': 0.3562005426002655, 'f': 0.4858350534266038, 'g': 0.01789803323610252}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 762us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:09,767]\u001b[0m Trial 29 finished with value: 12.052932093470368 and parameters: {'a': 0.00934312566744984, 'b': 0.3304901838557146, 'c': 0.41806283192642535, 'd': 0.6640875510669073, 'e': 0.26700463660293006, 'f': 0.3035288357323514, 'g': 0.004028235106139793}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 706us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:19,310]\u001b[0m Trial 30 finished with value: 11.880409331301964 and parameters: {'a': 0.09908926052216324, 'b': 0.3905898444145281, 'c': 0.07388203111656606, 'd': 0.9079563123555264, 'e': 0.12408632640565945, 'f': 0.18671059693620068, 'g': 0.18984140220158544}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 811us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:28,791]\u001b[0m Trial 31 finished with value: 11.83997878544801 and parameters: {'a': 0.249252813981088, 'b': 0.46611608629259943, 'c': 0.14569312644226518, 'd': 0.7859589356485488, 'e': 0.11173658115504537, 'f': 0.06101923300044321, 'g': 0.10181166683002647}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 815us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:38,785]\u001b[0m Trial 32 finished with value: 11.798306507881673 and parameters: {'a': 0.20978890864350944, 'b': 0.3957259963037081, 'c': 0.0637886407295125, 'd': 0.7725492753534619, 'e': 0.16389491067647244, 'f': 0.075770512081553, 'g': 0.0723438069470734}. Best is trial 26 with value: 11.708325200100223.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 678us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:48,389]\u001b[0m Trial 33 finished with value: 11.664632528211978 and parameters: {'a': 0.3673067573662826, 'b': 0.2686732010373094, 'c': 0.05824431372467316, 'd': 0.7082677963062497, 'e': 8.60276516286701e-05, 'f': 0.21571310926779147, 'g': 0.0003396876019698368}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 751us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:38:58,053]\u001b[0m Trial 34 finished with value: 11.783636721257366 and parameters: {'a': 0.36268597678107584, 'b': 0.29447016235826723, 'c': 0.06521220033170616, 'd': 0.6712026272204462, 'e': 0.026060713200651984, 'f': 0.2793520461999076, 'g': 0.15415031693888265}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 677us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:07,643]\u001b[0m Trial 35 finished with value: 11.73352531330699 and parameters: {'a': 0.14934946303561386, 'b': 0.24515906154545627, 'c': 0.13707864159534217, 'd': 0.9286296852553667, 'e': 0.07181691084863806, 'f': 0.20724285009499394, 'g': 0.005198245210384048}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 810us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:17,248]\u001b[0m Trial 36 finished with value: 11.780856555263181 and parameters: {'a': 0.2765094100050788, 'b': 0.2336738439186459, 'c': 0.15913087152857208, 'd': 0.9056798243445169, 'e': 0.08870888052099642, 'f': 0.1990118102479369, 'g': 0.06467011166911718}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 667us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:26,875]\u001b[0m Trial 37 finished with value: 11.974470765637767 and parameters: {'a': 0.13707140680009897, 'b': 0.1421851604238779, 'c': 0.11525983319937508, 'd': 0.9117808696641977, 'e': 0.0009592054250944504, 'f': 0.21710488888311735, 'g': 0.2917722798353139}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 678us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:36,319]\u001b[0m Trial 38 finished with value: 11.92947227202993 and parameters: {'a': 0.3033648941943202, 'b': 0.2351084927570352, 'c': 0.2369013877078084, 'd': 0.9444372159301789, 'e': 0.06721931606427783, 'f': 0.15947680051319485, 'g': 0.20323477519736222}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 681us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:45,844]\u001b[0m Trial 39 finished with value: 11.806098016181544 and parameters: {'a': 0.371914262792855, 'b': 0.31472243513669373, 'c': 0.16497403063117055, 'd': 0.874674036609691, 'e': 0.07379727132866021, 'f': 0.3253094318454718, 'g': 0.13713712129166367}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 666us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:39:55,825]\u001b[0m Trial 40 finished with value: 11.75275345052804 and parameters: {'a': 0.3264844565129321, 'b': 0.24085898157225027, 'c': 0.06371683330478564, 'd': 0.7000654424163513, 'e': 0.13765552747618734, 'f': 0.2632766348232807, 'g': 0.06411088237901359}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 680us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:05,663]\u001b[0m Trial 41 finished with value: 11.769203142660132 and parameters: {'a': 0.1633408207553041, 'b': 0.23895199200607659, 'c': 0.05874171601432838, 'd': 0.6315466573540449, 'e': 0.14460084750613056, 'f': 0.3681317547717663, 'g': 0.05672489143999805}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 674us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:15,079]\u001b[0m Trial 42 finished with value: 11.740648540779915 and parameters: {'a': 0.3250295856184298, 'b': 0.18833736812424495, 'c': 0.1291716643586049, 'd': 0.7241631335329818, 'e': 0.04172940584817924, 'f': 0.25983130409558824, 'g': 0.047839683934517124}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 678us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:24,990]\u001b[0m Trial 43 finished with value: 11.78390751787535 and parameters: {'a': 0.3182309486988859, 'b': 0.1745706705577193, 'c': 0.12890860974822005, 'd': 0.543144798908457, 'e': 0.047937900850162224, 'f': 0.2507121410474449, 'g': 0.06874421857207488}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 741us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:34,351]\u001b[0m Trial 44 finished with value: 11.80758401353983 and parameters: {'a': 0.4086823141305033, 'b': 0.09790956142836563, 'c': 0.09674863176210019, 'd': 0.48034778000639, 'e': 0.08957396372183775, 'f': 0.29121093176426865, 'g': 0.10523006289907828}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 698us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:43,920]\u001b[0m Trial 45 finished with value: 11.853581614636623 and parameters: {'a': 0.2485489711430428, 'b': 0.15556070176623313, 'c': 0.14084537385447787, 'd': 0.6920155833409324, 'e': 0.024689830144838085, 'f': 0.3918644072000951, 'g': 0.17169309313837233}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 781us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:40:53,460]\u001b[0m Trial 46 finished with value: 11.729011178374156 and parameters: {'a': 0.34347629138706315, 'b': 0.21702474642665895, 'c': 0.06444821009133317, 'd': 0.71169581566862, 'e': 0.15171092346258003, 'f': 0.3374530213536516, 'g': 0.03841530796922996}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 753us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:03,027]\u001b[0m Trial 47 finished with value: 11.825388087102906 and parameters: {'a': 0.17344755721190144, 'b': 0.07522226895134873, 'c': 0.18380172212627952, 'd': 0.9451026563199985, 'e': 0.1897829190690722, 'f': 0.33310332251525965, 'g': 0.05068421776370267}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 726us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:13,064]\u001b[0m Trial 48 finished with value: 11.813530519455357 and parameters: {'a': 0.37976640434114206, 'b': 0.21013605563988624, 'c': 0.25480645927811196, 'd': 0.6152114530852713, 'e': 0.0018242703713329546, 'f': 0.2079262550898695, 'g': 0.04442723110619556}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:23,325]\u001b[0m Trial 49 finished with value: 11.796623043016057 and parameters: {'a': 0.28201017394619704, 'b': 0.18588333851966674, 'c': 0.342128857213206, 'd': 0.9834174107162754, 'e': 0.061724745610890745, 'f': 0.4282230715313579, 'g': 0.000508123529074379}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 690us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:34,207]\u001b[0m Trial 50 finished with value: 11.830904586003806 and parameters: {'a': 0.4376829097086988, 'b': 0.06673044804035581, 'c': 0.1230781488926116, 'd': 0.8379143914484122, 'e': 0.23194184197549822, 'f': 0.3799572942725015, 'g': 0.1245703939018789}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 674us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:43,922]\u001b[0m Trial 51 finished with value: 11.74300060783216 and parameters: {'a': 0.3353118309336301, 'b': 0.24517146331700634, 'c': 0.06500759567444969, 'd': 0.7207863471603198, 'e': 0.15235432183655973, 'f': 0.2681159173443784, 'g': 0.04897160668837583}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 698us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:41:54,223]\u001b[0m Trial 52 finished with value: 11.733115408017174 and parameters: {'a': 0.3435154513013325, 'b': 0.14538045152953502, 'c': 0.062000693305252874, 'd': 0.7802096027820516, 'e': 0.17651618969795965, 'f': 0.3055799002136585, 'g': 0.03867127745863355}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 764us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:03,810]\u001b[0m Trial 53 finished with value: 11.7360859641868 and parameters: {'a': 0.41819281573627964, 'b': 0.13784771498804277, 'c': 0.040348222364947284, 'd': 0.7802317574920675, 'e': 0.10858319631570096, 'f': 0.3045953694365543, 'g': 0.09925598042422554}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 653us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:13,748]\u001b[0m Trial 54 finished with value: 11.728508665948794 and parameters: {'a': 0.40729773461484675, 'b': 0.1375844255954164, 'c': 0.04737920584622661, 'd': 0.7801504994345444, 'e': 0.10466356709148648, 'f': 0.32109477835737893, 'g': 0.08648844712338231}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 645us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:23,561]\u001b[0m Trial 55 finished with value: 11.779993383987241 and parameters: {'a': 0.45730697944760024, 'b': 0.04936412263773826, 'c': 0.03426747212859952, 'd': 0.8021002304124085, 'e': 0.1725316003780304, 'f': 0.35491943357948375, 'g': 0.1397909438211235}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 644us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:32,978]\u001b[0m Trial 56 finished with value: 11.744553139765951 and parameters: {'a': 0.37770486859507335, 'b': 0.11872552010082293, 'c': 0.09172411818575467, 'd': 0.8580218897553135, 'e': 0.2320960574399028, 'f': 0.13483015229367207, 'g': 0.0026227975953487526}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 664us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:42,910]\u001b[0m Trial 57 finished with value: 11.787152772979399 and parameters: {'a': 0.2629482475522008, 'b': 0.1591846417931963, 'c': 0.19385689418186464, 'd': 0.8247335940257868, 'e': 0.08924779175927842, 'f': 0.22291299428016378, 'g': 0.032390048142451305}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 744us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:42:52,380]\u001b[0m Trial 58 finished with value: 11.794253604968922 and parameters: {'a': 0.23207684105992848, 'b': 0.20331788537059042, 'c': 0.030332902809809506, 'd': 0.7595154193661684, 'e': 0.17292331652219273, 'f': 0.16166668695693254, 'g': 0.09018131603245172}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 685us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:01,850]\u001b[0m Trial 59 finished with value: 11.85068664892418 and parameters: {'a': 0.3988259331097571, 'b': 0.30954562063926605, 'c': 0.1746468793042648, 'd': 0.9377778037861491, 'e': 0.14394534395780362, 'f': 0.3216363911077952, 'g': 0.17826434531618515}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 660us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:11,409]\u001b[0m Trial 60 finished with value: 11.758732449371342 and parameters: {'a': 0.19355488494268036, 'b': 0.2724718432537271, 'c': 0.09503553092263128, 'd': 0.9966641314862316, 'e': 0.2261260237207411, 'f': 0.40100751534826495, 'g': 0.03802962864720594}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 646us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:21,040]\u001b[0m Trial 61 finished with value: 11.752412608555659 and parameters: {'a': 0.429483103190492, 'b': 0.13982371744170974, 'c': 0.03537502885483188, 'd': 0.7618237660977267, 'e': 0.10477326777943137, 'f': 0.2992748169546355, 'g': 0.12356742286018113}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 687us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:30,415]\u001b[0m Trial 62 finished with value: 11.72804385157148 and parameters: {'a': 0.35984350426423206, 'b': 0.1365705107151872, 'c': 0.0347081784399671, 'd': 0.7998713049294637, 'e': 0.10721224596585231, 'f': 0.35652345818923803, 'g': 0.09024171554364342}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 705us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:40,148]\u001b[0m Trial 63 finished with value: 11.686950773208816 and parameters: {'a': 0.3371074724602856, 'b': 0.09007429961689797, 'c': 0.0078843045270003, 'd': 0.8693704467087678, 'e': 0.03910539179164685, 'f': 0.3445739491285085, 'g': 0.08251342749472562}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 676us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:49,774]\u001b[0m Trial 64 finished with value: 11.67613755019842 and parameters: {'a': 0.36876082583759884, 'b': 0.03809146751321599, 'c': 0.0018238199981203754, 'd': 0.8141509477201015, 'e': 0.02457850412039883, 'f': 0.43265460363315994, 'g': 0.08285197871050912}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 646us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:43:59,359]\u001b[0m Trial 65 finished with value: 11.676923647050689 and parameters: {'a': 0.4705652579255102, 'b': 0.025984413631933122, 'c': 0.009199450568813078, 'd': 0.8773123100363115, 'e': 0.039834873043413266, 'f': 0.45338643166240367, 'g': 0.09070447778818977}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 680us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:08,867]\u001b[0m Trial 66 finished with value: 11.766193275414913 and parameters: {'a': 0.4809740301645844, 'b': 0.029966396059388115, 'c': 0.005035244469416226, 'd': 0.8817008577383643, 'e': 0.027231080602800398, 'f': 0.4317403681961014, 'g': 0.2264436803729497}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 697us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:18,417]\u001b[0m Trial 67 finished with value: 11.72677047287334 and parameters: {'a': 0.5302957512891208, 'b': 0.09064714053606142, 'c': 0.02175119795259904, 'd': 0.8138341455559549, 'e': 0.05689896679193955, 'f': 0.4632351736474618, 'g': 0.154229771509825}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 634us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:27,929]\u001b[0m Trial 68 finished with value: 11.704642283694035 and parameters: {'a': 0.5417882751062332, 'b': 0.02319981978468111, 'c': 0.0025699277130980525, 'd': 0.8666014900633977, 'e': 0.031869802424710075, 'f': 0.509284450697053, 'g': 0.1532814356664861}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 658us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:37,474]\u001b[0m Trial 69 finished with value: 11.742042793281628 and parameters: {'a': 0.5697834006251068, 'b': 0.03350476280842698, 'c': 0.09754142006435906, 'd': 0.890395323378264, 'e': 0.02068661028415157, 'f': 0.46853064582675413, 'g': 0.15887969463980173}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 696us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:47,000]\u001b[0m Trial 70 finished with value: 11.740637064267398 and parameters: {'a': 0.6236443641975264, 'b': 0.001814910321294827, 'c': 0.003665421483797948, 'd': 0.8489819756662804, 'e': 0.03133786421808092, 'f': 0.4659642850404552, 'g': 0.21396316774337185}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 648us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:44:56,532]\u001b[0m Trial 71 finished with value: 11.674296592880541 and parameters: {'a': 0.519404665751487, 'b': 0.07443713576225164, 'c': 0.027783351049953796, 'd': 0.816852808293665, 'e': 0.0005484743413368687, 'f': 0.5162599679635194, 'g': 0.08930968394115117}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 723us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:06,217]\u001b[0m Trial 72 finished with value: 11.70903169152609 and parameters: {'a': 0.5252578434855492, 'b': 0.0883245548685608, 'c': 0.0036836149408623134, 'd': 0.8709026260566255, 'e': 0.06413663195127622, 'f': 0.5197763450200039, 'g': 0.14284869346309229}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 734us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:15,801]\u001b[0m Trial 73 finished with value: 11.679588556983203 and parameters: {'a': 0.5104258883123116, 'b': 0.055955685602853834, 'c': 0.004797964482623398, 'd': 0.8624135630530406, 'e': 0.00824599274829913, 'f': 0.5548269514836158, 'g': 0.11447743470292293}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 764us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:26,019]\u001b[0m Trial 74 finished with value: 11.707788345522392 and parameters: {'a': 0.5020897734637599, 'b': 0.0515347190072363, 'c': 0.09052550988172461, 'd': 0.9239612409882992, 'e': 0.0031928250271802804, 'f': 0.5325365801874695, 'g': 0.11247053160672196}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 698us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:35,738]\u001b[0m Trial 75 finished with value: 11.72932091050844 and parameters: {'a': 0.4966395558196852, 'b': 0.05401716200407996, 'c': 0.03473731260700009, 'd': 0.9556929817280646, 'e': 0.006947741965231076, 'f': 0.5397295665990864, 'g': 0.18524555303468904}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 630us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:45,491]\u001b[0m Trial 76 finished with value: 11.71594348533642 and parameters: {'a': 0.4653928315372802, 'b': 0.031315144104006695, 'c': 0.08552901845456895, 'd': 0.8374622895434806, 'e': 0.043723237830572234, 'f': 0.6059294160791852, 'g': 0.10544674670143714}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 656us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:45:54,999]\u001b[0m Trial 77 finished with value: 11.695939781962855 and parameters: {'a': 0.5473166527220431, 'b': 0.0636970091720766, 'c': 0.11312385703308644, 'd': 0.9292075336371877, 'e': 0.004610885600249495, 'f': 0.568362105513521, 'g': 0.08240664258287882}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 701us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:04,577]\u001b[0m Trial 78 finished with value: 11.670108677389127 and parameters: {'a': 0.5650554166263393, 'b': 0.10313512041458439, 'c': 0.028371111766994264, 'd': 0.9655383448113154, 'e': 0.04150224407745557, 'f': 0.49155404628431604, 'g': 0.07526891588777723}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 688us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:14,203]\u001b[0m Trial 79 finished with value: 11.71025609262854 and parameters: {'a': 0.5880954451818838, 'b': 0.10462135890317706, 'c': 0.12233556680513478, 'd': 0.9745764233712128, 'e': 0.07531737608979547, 'f': 0.567693289140263, 'g': 0.0825166276718243}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 635us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:23,795]\u001b[0m Trial 80 finished with value: 11.674783172066467 and parameters: {'a': 0.5597081988956749, 'b': 0.07142376342602003, 'c': 0.0464483866489736, 'd': 0.9265453450888884, 'e': 0.04725531172479614, 'f': 0.4936284044571636, 'g': 0.07027704578646063}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 655us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:33,396]\u001b[0m Trial 81 finished with value: 11.669545084011304 and parameters: {'a': 0.5702355509871112, 'b': 0.08632117419478472, 'c': 0.051021617489403356, 'd': 0.9238045626389362, 'e': 0.0003943591273300193, 'f': 0.49326923543117523, 'g': 0.07438180113196426}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 721us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:42,856]\u001b[0m Trial 82 finished with value: 11.671439792008282 and parameters: {'a': 0.6137613280783181, 'b': 0.08500201173751644, 'c': 0.0432602444029421, 'd': 0.8878881038783504, 'e': 0.04236568482860518, 'f': 0.48894515525964993, 'g': 0.07129921073705811}. Best is trial 33 with value: 11.664632528211978.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 671us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:46:52,307]\u001b[0m Trial 83 finished with value: 11.66310904106663 and parameters: {'a': 0.6596083336621402, 'b': 8.901958810539345e-05, 'c': 0.050786606318048697, 'd': 0.8905044868184925, 'e': 0.05230839141388126, 'f': 0.4963605739823891, 'g': 0.058833339951855475}. Best is trial 83 with value: 11.66310904106663.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 707us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:01,796]\u001b[0m Trial 84 finished with value: 11.649441856319006 and parameters: {'a': 0.6591382275511279, 'b': 0.002679222243752697, 'c': 0.05112651390207882, 'd': 0.8941115849385034, 'e': 0.08358866945680773, 'f': 0.4905956384268291, 'g': 0.018263638437283347}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 734us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:11,282]\u001b[0m Trial 85 finished with value: 11.657465435671954 and parameters: {'a': 0.6718930730947308, 'b': 0.11560747648293829, 'c': 0.050139775931650137, 'd': 0.9752641060093908, 'e': 0.08237431217290145, 'f': 0.49226865615424376, 'g': 0.02843597207670167}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 779us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:21,185]\u001b[0m Trial 86 finished with value: 11.665779113716043 and parameters: {'a': 0.6900876977397372, 'b': 0.11429951585937363, 'c': 0.05344968096272884, 'd': 0.9668100754122989, 'e': 0.13079479641417738, 'f': 0.49170146329725284, 'g': 0.026370415105122502}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 644us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:30,818]\u001b[0m Trial 87 finished with value: 11.695836956075363 and parameters: {'a': 0.6812962215076424, 'b': 0.11534452830242851, 'c': 0.15032291679881998, 'd': 0.9643616570295142, 'e': 0.1294118959242239, 'f': 0.6309326506246837, 'g': 0.025389503199226213}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 671us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:40,369]\u001b[0m Trial 88 finished with value: 11.657680576259677 and parameters: {'a': 0.6292752623631067, 'b': 0.0017644892110065258, 'c': 0.0757164467349745, 'd': 0.9906239108751533, 'e': 0.07764384607446881, 'f': 0.48760636238154925, 'g': 0.020081690388108177}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 668us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:50,075]\u001b[0m Trial 89 finished with value: 11.662003958312303 and parameters: {'a': 0.6251842479002568, 'b': 0.007451215623687103, 'c': 0.08346478383923148, 'd': 0.987237324715925, 'e': 0.07490675249545702, 'f': 0.48235245494872026, 'g': 0.023376637494730457}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 869us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:47:59,647]\u001b[0m Trial 90 finished with value: 11.670515486868622 and parameters: {'a': 0.6577973022896557, 'b': 0.006827868918419938, 'c': 0.07702373515928712, 'd': 0.9976780531759114, 'e': 0.1317471863449285, 'f': 0.40395359026998634, 'g': 0.023433426642349327}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 728us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:09,317]\u001b[0m Trial 91 finished with value: 11.656807751952531 and parameters: {'a': 0.6791123648952044, 'b': 0.005894841734513501, 'c': 0.07459076439495356, 'd': 0.9970818767841235, 'e': 0.08241758532213402, 'f': 0.4173982311473581, 'g': 0.022055537165429902}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 825us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:19,031]\u001b[0m Trial 92 finished with value: 11.669065610189394 and parameters: {'a': 0.710905169742039, 'b': 0.0008564256393738708, 'c': 0.11073693772768517, 'd': 0.9678322384366744, 'e': 0.08156659104363946, 'f': 0.4874800761486068, 'g': 0.024839685731950608}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 856us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:28,743]\u001b[0m Trial 93 finished with value: 11.688569491681491 and parameters: {'a': 0.721871987501778, 'b': 0.009953555729028967, 'c': 0.15502499710831535, 'd': 0.9817302273288586, 'e': 0.09399817345616666, 'f': 0.4118311939180028, 'g': 0.024044636344031166}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 651us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:38,403]\u001b[0m Trial 94 finished with value: 11.658244857301716 and parameters: {'a': 0.6898495298078374, 'b': 0.011407699018475631, 'c': 0.10966828427958837, 'd': 0.9559263312782915, 'e': 0.07576466146769655, 'f': 0.44958315159124373, 'g': 0.0008620741050039127}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 649us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:47,872]\u001b[0m Trial 95 finished with value: 11.661850432525226 and parameters: {'a': 0.7020418551851736, 'b': 0.006781111132670246, 'c': 0.1176862725408091, 'd': 0.9454042257598099, 'e': 0.08321947830845453, 'f': 0.45219545647743775, 'g': 0.0007031152353028092}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 648us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:48:57,234]\u001b[0m Trial 96 finished with value: 11.657472162539136 and parameters: {'a': 0.7437292322248336, 'b': 0.02164680359023813, 'c': 0.0771336358888187, 'd': 0.9526922592143319, 'e': 0.13250511706892268, 'f': 0.44817862757375226, 'g': 0.0043834181229356894}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 793us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:49:06,663]\u001b[0m Trial 97 finished with value: 11.667048445845857 and parameters: {'a': 0.7660275312503974, 'b': 0.022846527225836084, 'c': 0.1347297983529485, 'd': 0.9955969291039412, 'e': 0.08089126844637294, 'f': 0.4486931842543316, 'g': 0.0079169572375557}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 654us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:49:16,422]\u001b[0m Trial 98 finished with value: 11.689687244853461 and parameters: {'a': 0.7537781208173255, 'b': 0.049956791208720444, 'c': 0.17181941746572038, 'd': 0.9048105491822664, 'e': 0.11335872276735338, 'f': 0.37908229498812085, 'g': 5.2599374778507235e-05}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 0s 816us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-11 16:49:26,089]\u001b[0m Trial 99 finished with value: 11.668196811985892 and parameters: {'a': 0.6486899344762496, 'b': 0.001113515470054979, 'c': 0.07574341869874919, 'd': 0.9514376811386647, 'e': 0.0634464656736166, 'f': 0.4173299515210281, 'g': 0.04669873641669166}. Best is trial 84 with value: 11.649441856319006.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 0.6591382275511279,\n",
       " 'b': 0.002679222243752697,\n",
       " 'c': 0.05112651390207882,\n",
       " 'd': 0.8941115849385034,\n",
       " 'e': 0.08358866945680773,\n",
       " 'f': 0.4905956384268291,\n",
       " 'g': 0.018263638437283347}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def coef_objective(trial):\n",
    "    a = trial.suggest_float('a', 0, 1)\n",
    "    b = trial.suggest_float('b', 0, 1)\n",
    "    c = trial.suggest_float('c', 0, 1)\n",
    "    d = trial.suggest_float('d', 0, 1)\n",
    "    e = trial.suggest_float('e', 0, 1)\n",
    "    f = trial.suggest_float('f', 0, 1)\n",
    "    g = trial.suggest_float('g', 0, 1)\n",
    "\n",
    "    preds_eval = []\n",
    "    for model in modelsXGB:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resXGB = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    preds_eval = []\n",
    "    for model in modelsCB:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resCB = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    preds_eval = []\n",
    "    for model in modelsLasso:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resLasso = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    preds_eval = []\n",
    "    for model in modelsLGBM:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resLGBM = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    preds_eval = []\n",
    "    for model in modelsSVR:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resSVR = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    preds_eval = []\n",
    "    for model in modelsRF:\n",
    "        preds_eval.append(model.predict(X))\n",
    "    \n",
    "    resRF = np.average(np.array(preds_eval),axis=0)\n",
    "    \n",
    "    resNN = np.average(np.array(NN_model.predict(X)), axis = 0)\n",
    "    \n",
    "    res1 = (resXGB * a + resCB * b + resLasso * c + resLGBM * d + resSVR * e + resRF * f + resNN * g)/(a + b + c + d + e + f + g)\n",
    "    \n",
    "    res = mean_squared_error(y, res1, squared=False)\n",
    "\n",
    "    return res\n",
    "\n",
    "study = optuna.create_study(direction= 'minimize')\n",
    "study.optimize(coef_objective, n_trials= 100)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 11.632224313221586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = study.best_params['a']\n",
    "b = study.best_params['b']\n",
    "c = study.best_params['c']\n",
    "d = study.best_params['d']\n",
    "e = study.best_params['e']\n",
    "f = study.best_params['f']\n",
    "g = study.best_params['g']\n",
    "\n",
    "# a = 0.8548784784620398\n",
    "# b = 0.057459781784847896\n",
    "# c = 0.04093386132430907\n",
    "# d = 0.049843870753921445\n",
    "# e = 0.05028312502340519\n",
    "# f = 0.03687994916292475\n",
    "\n",
    "sum_coef = a + b + c + d + e + f + g\n",
    "a = a / sum_coef\n",
    "b = b / sum_coef\n",
    "c = c / sum_coef\n",
    "d = d / sum_coef\n",
    "e = e / sum_coef\n",
    "f = f / sum_coef\n",
    "g = g / sum_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predXGB * a + predCB * b + predLasso * c + predLGBM * d + predSVR * e + predRF * f + predNN * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-07T18:10:26.297713Z",
     "iopub.status.busy": "2023-03-07T18:10:26.297325Z",
     "iopub.status.idle": "2023-03-07T18:10:26.303468Z",
     "shell.execute_reply": "2023-03-07T18:10:26.301786Z",
     "shell.execute_reply.started": "2023-03-07T18:10:26.297677Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds, columns = ['Strength']).set_index(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission.Strength.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>48.652984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>20.264379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>34.191657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>47.180216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5411</th>\n",
       "      <td>33.169046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9007</th>\n",
       "      <td>32.970034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9008</th>\n",
       "      <td>35.728368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9009</th>\n",
       "      <td>38.786611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9010</th>\n",
       "      <td>32.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9011</th>\n",
       "      <td>21.348134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3605 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Strength\n",
       "id             \n",
       "5407  48.652984\n",
       "5408  20.264379\n",
       "5409  34.191657\n",
       "5410  47.180216\n",
       "5411  33.169046\n",
       "...         ...\n",
       "9007  32.970034\n",
       "9008  35.728368\n",
       "9009  38.786611\n",
       "9010  32.004945\n",
       "9011  21.348134\n",
       "\n",
       "[3605 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
